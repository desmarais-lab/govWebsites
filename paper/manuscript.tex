\documentclass[11pt]{article}

%==============Packages & Commands==============
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{tikz}
%%%<
\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{tikzpicture}
%\setlength\PreviewBorder{5pt}%

\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
% \geometry{a4paper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activat\usetikzlibrary{arrows}e for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{arrows}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage[longnamesfirst]{natbib} % For references
\bibpunct{(}{)}{;}{a}{}{,} % Reference punctuation
\usepackage{changepage}
\usepackage{setspace}
\usepackage{booktabs} % For tables
\usepackage{rotating} % For sideways tables/figures
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{comment}
%\usepackage{fullwidth}
\newcolumntype{d}[1]{D{.}{\cdot}{#1}}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{3}{D{.}{.}{3}}
\newcolumntype{4}{D{.}{.}{4}}
\newcolumntype{5}{D{.}{.}{5}}
\usepackage{float}
\usepackage[hyphens]{url}
%\usepackage[margin = 1.25in]{geometry}
%\usepackage[nolists,figuresfirst]{endfloat} % Figures and tables at the end
\usepackage{subfig}
\captionsetup[subfloat]{position = top, font = normalsize} % For sub-figure captions
\usepackage{fancyhdr}
%\makeatletter
%\def\url@leostyle{%
%  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
%\makeatother
%% Now actually use the newly defined style.
\urlstyle{same}
\usepackage{times}

\usepackage{lscape}
% \usepackage{mathptmx}
%\usepackage[colorlinks = true,
%						bookmarksopen = true,
%						pagebackref = true,
%						linkcolor = black,
%						citecolor = black,
% 					urlcolor = black]{hyperref}
%\usepackage[all]{hypcap}
%\urlstyle{same}
\newcommand{\fnote}[1]{\footnote{\normalsize{#1}}} % 12 pt, double spaced footnotes
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\def\citeaposs#1{\citeauthor{#1}' (\citeyear{#1})}
\newcommand{\bm}[1]{\boldsymbol{#1}} %makes bold math symbols easier
\newcommand{\R}{\textsf{R}\space} %R in textsf font
\newcommand{\netinf}{\texttt{NetInf}\space} %R in textsf font
\newcommand{\iid}{i.i.d} %shorthand for iid
\newcommand{\cites}{{\bf \textcolor{red}{CITES}}} %shorthand for iid
%\usepackage[compact]{titlesec}
%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\bibsep}{2pt}
%\renewcommand{\headrulewidth}{0pt}

%\renewcommand{\figureplace}{ % This places [Insert Table X here] and [Insert Figure Y here] in the text
%\begin{center}
%[Insert \figurename~\thepostfig\ here]
%\end{center}}
%\renewcommand{\tableplace}{%
%\begin{center}
%[Insert \tablename~\theposttbl\ here]
%\end{center}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Y}{\bm{\mathcal{Y}}}
\newcommand{\bZ}{\bm{Z}}

\usepackage[colorlinks = TRUE, urlcolor = black, linkcolor = black, citecolor = black, pdfstartview = FitV]{hyperref}


%============Article Title, Authors==================
\title{\vspace{-2cm} The effects of transitions of power on the contents of municipal government websites }


\author{ Markus Neumann \and Bruce Desmarais \and Hanna Wallach \and Fridolin Linder} \date{\today}



%===================Startup=======================
\begin{document}
\maketitle



%=============Abstract & Keywords==================

\begin{abstract}

Websites have become a prominent source of data for automated text analysis in political science. However, extant research lacks common standards and often glosses over the details on how such analyses are conducted, exposing itself to potential pitfalls associated with this process and making replication difficult. We develop a set of guidelines and procedures to be followed in order to produce valid results. In order to develop a valid research design, the appropriate selection of cases and URLs is crucial. For the acquisition of website data, we cover several scraping methods and difficulties that can arise in the process. Pre-processing is a common step in text analysis, but when websites are concerned, additional measures need to be taken in order to guard against potential sources of bias. Finally, we cover several methods of analysis and validation appropriate for this kind of data. These steps are illustrated through our creation and exploitation of a new and innovative dataset - the websites of local governments in Indiana and Louisiana. We show that if our methodology is followed appropriately, an association between mayoral partisanship and the content of their cities' websites becomes visible.

%\noindent We explore the effect of transitions of power in municipal governments on the content of their websites. We hypothesize that when party control changes, city administrators modify the contents of their websites in order to fit the agenda of the new incumbent. To test this theory, we study cities in Indiana and Louisiana, two states in which all municipal elections are partisan and the parties of the candidates appear on the ballots. Snapshots of websites before and after transitions of power are acquired through the Wayback Machine. We apply statistical topic models based in latent dirichlet allocation, focusing on changes to the websites. We present results on both which topics see the greatest degree of change associated with transitions in city administrations, and how the topics modified differ with regard to political parties.

\end{abstract}
\thispagestyle{empty}
% \doublespacing
% Description of the possible challenges
\section{Introduction}

\subsection{New Research Design}
The analysis of entire websites has become more prominent recently, especially in the the text-as-data movement. We see great promise in this development, especially as it pertains to the study of governmental branches and agencies, which are often resilient to being studied by other methods. However, this line of inquiry comes with a set of particular challenges and pitfalls researchers need to be mindful of. We offer solutions to these problems at the four stages of such an analysis.

\begin{enumerate}
	\item Design
	\begin{enumerate}
		\item Choosing the sample
		\item Finding URLs
		\item list of .gov websites
		\item Finding supporting data
	\end{enumerate}
	\item Scraping
		\begin{enumerate}
			\item wget
			\item headless browser/Selenium
			\item Beautifulsoup/rvest
			\item APIs (httr)
			\item Wayback Machine
		\end{enumerate}
	\item Pre-processing
		\begin{enumerate}
			\item Determining document filetype
			\item File conversion
			\item Conventional preprocessing (lowercase, numbers, punctuation)
			\item Stemming and lemmatization
			\item spellchecking
			\item Dealing with duplicate text \& html documents in particular
		\end{enumerate}
	\item Analysis
		\begin{enumerate}
			\item LDA
			\item Other topic models (structural, author, dynamic -- maybe?)
			\item SVM (+ other machine learning classifiers?)
			\item Fightin Words
		\end{enumerate}
\end{enumerate}

\section{Background}

\cite{grimmelikhuijsen2010transparency} run an experiment in which citizens are exposed to randomly selected levels of information about local government council minutes. They find a negative relationship between the information level and perceptions of competence in the local government. This raises an interesting question regarding whether citizens or more likely to participate when they perceive competence or when they perceive incompetence.

\cite{wang2005evaluating} present a widely cited `model' for evaluating the accessibility of information on government websites. This is an important paper with which we should be familiar at a very detailed level as we use archived web content to assess the volume/accessibility of information provided by local governments.

\cite{osman2014cobra} is less relevant, but they develop a multi-item measure to predict the level of citizen satisfaction with e-government services. 


\cite{grimmelikhuijsen2012developing} conduct an enormously relevant study. Insofar as we analyze what predicts openness of government websites, we will be replicating and building upon this study. They focus on Dutch municipal websites, and their approach is fairly limited in scope and highly manual (which we can compliment). For example, one of the dependent variables ``Decision-making transparency,'' is measured ``using a discrete (1/0) indicator for whether the underlying principles or reasons for local air pollution policies were given on the Web site.'' 

\begin{landscape}
\begin{table}[htbp]
	%\caption{}
	\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
		\hline
		Names & Year & Journal & Findings & Important? \\ \hline
		Benedictis-Kessner, Justin De
		Warshaw, Christopher & 2016 & JOP* & Regression discontinuity design. Democratic mayors spend more (but it is unclear on what, not the typical Democratic issue-areas), issue more debt, pay more interest & Yes \\ \hline
		Caughey, Devin
		Warshaw, Christopher
		Xu, Yiqing & 2015 & Working Paper & Regression discontinuity design. Partisan composition of state governments affects state policy liberalism (composite index for the areas of social welfare, taxation, labor, civil rights, women’s rights, moral legislation, family planning, environment). & Somewhat \\ \hline
		Einstein, Katherine Levine
		Kogan, Vladimir & 2015 & Urban Affairs Review & Cities with more Democratic citizens spend more; more progressive (rather than regressive) forms of taxation; pursue intergov. aid more; spend more on police, fire, parks \& recreation & Somewhat \\ \hline
		Einstein, Katherine Levine
		Glick, David M. & 2015 & Working Paper & Survey of 72 mayors. Unlike Republican mayors, roughly half of Democrats seem to agree that cities should aim to reduce inequality. Democratic mayors also seem to favor redistribution to accomplish that goal. & Somewhat \\ \hline
		Kiewiet, D Roderick
		Mccubbins, Mathew D & 2014 & Annual Review & City budgets have been severeley constrained since the Great Recession. Spending has thus decreased in general. Lack of funds means that there is not much discretion for partisanship. & Somewhat \\ \hline
		Tausanovitch, Chris
		Warshaw, Christopher & 2014 & APSR* & Cities are responsive (taxes, expenditures, regressiveness of taxation) to citizens' conservatism/liberalism. Partisan elections do not make cities more or less responsive. & Yes \\ \hline
		Guillamón, Ma Dolores
		Bastida, Francisco
		Benito, Bernardino & 2013 & European Journal of Law and Economics & Police spending in Spain. Conservative parties spend more on police. Spending is higher before elections. Also contains a useful overview of the literature. & Yes \\ \hline
	\end{tabular}
	\label{}
\end{table}
\end{landscape}

\begin{landscape}
	\begin{table}[htbp]
		%\caption{}
		\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
			\hline
			Names & Year & Journal & Findings & Important? \\ \hline
			Gerber, Elisabeth R. & 2013 & Cityscape & Partisanship of both citizens and elected city officials separately affect climate policy. & Yes \\ \hline
			Solé-Ollé, Albert
			Viladecans-Marsal, Elisabet & 2013 & Journal of Urban Economics & Spanish cities. The authors "employ a regression discontinuity design to document that cities controlled by left-wing parties convert much less land from rural to urban uses than is the case in similar cities con- trolled by the right". Partisanship might also affect housing construction and price growth. & Yes \\ \hline
			Gerber, Elisabeth R.
			Hopkins, Daniel J. & 2011 & AJPS & Regression discontinuity design. Democratic mayors spend less on public safety. All other policy areas (including taxation) are unaffected. & Yes \\ \hline
			Trounstine, Jessica & 2010 & Annual Review & Race and ethnicity in local elections (not relevant to us). Partisan elections have higher turnout; non-partisan elections still tend to have some partisanship in them because voters learn about party of candidates from media. Non-partisan elections favor Republicans/upper class. Mixed evidence for whether partisanship of mayor is important for policy. & Somewhat \\ \hline
			Palus, Christine Kelleher & 2010 & State and Local Government Review & Ideology (liberal/conservative) of citizen is well represented by gov. spending in five areas: (1) community development, housing, and conservation, (2) health and human services, (3) culture, the arts, and recreation, (4) environmental programs, and (5) transportation. & Somewhat \\ \hline
			Ferreira, Fernando
			Gyourko, Joseph & 2009 & The Quarterly Journal of Economics & Regression discontinuity design. Null results for spending and city gov. size with regard to mayor partisanship. & Yes \\ \hline
			Ansolabehere, Stephen
			Snyder, James M. & 2006 & Scandinavian Journal of Economics & Despite the journal, this is about the U.S. The important finding (for us) is the fact that counties whose government is controlled by the same party as the state government, receive more funding (county's share of state transfers, normalized by county pop.) from the state. & Somewhat \\ \hline
			Murphy, Russell D. & 2002 & Annual Review & Not useful. Too philosophical; mostly cites papers written a hundred years ago. Also exclusively about larger cities. & No \\ \hline
		\end{tabular}
		\label{}
	\end{table}
\end{landscape}

\begin{landscape}
	\begin{table}[htbp]
		%\caption{}
		\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
			\hline
			Names & Year & Journal & Findings & Important? \\ \hline
			Armstrong, Cory L. & 2011 & Government Information Quarterly & Comparison of county and school board websites in Florida (where the two align) with regard to transparency (presence or absence of public records). Manual content analysis (undergrads told to look around for 15 minutes). School board websites, more professional websites, and websites in Republican-dominated counties are found to be more transparent. & Yes \\ \hline
			Cegarra-Navarro, Juan
			Pachón, José
			Cegarra, José & 2012 & International Journal of Information Management & Survey of Spanish municipal government officials (specifically, the city website managers). Respondents are asked about the features of their websites, the level of civic engagement and the size of their municipality. More sophisticated websites are correlated with greater civic engagement and greater use of e-government functions. & Yes \\ \hline
			Dolson, Jordan
			Young, Robert & 2012 & Canadian Journal of Urban Research & Determinants of website content. Three categories: e-content (city information on website), e-participation, social media use. Tables on page 15 show frequencies of these categories across sites, and might be useful to inform our topics. Larger cities have better websites. Population growth and immigration are also tested, but the findings are somewhat inconclusive. & Yes \\ \hline
			Feeney, Mary K.
			Brown, Adrian & 2017 & Government Information Quarterly & 500 U.S. city websites at two points in time (2010-2014). Count model of website features regarding information, e-services, utilities, transparency and civic engagement. Having a larger population leads to more features. Relying on a website contractor leads to more information and transparency. The authors say that mayor-councils are negatively correlated with website sophistication, but their regression tables state the opposite. & Yes \\ \hline
			Kaylor, Charles
			Deshazo, Randy
			Van Eck, David & 2001 & Government Information Quarterly & Model of best practices of e-government. Table 1 lists a number of possible ways this manifests, could be useful for our theory. & Somewhat \\ \hline
			Ansolabehere, Stephen
			Urban, Florian & 2002 & Cities & Websites of 20 major cities across the world. Is website content correlated with city characteristics? Not particularly systematic, and the findings are inconclusive. & Somewhat \\ \hline
			Jeffres, Leo W.
			Lin, Carolyn A. & 2006 & Journal of Computer-Mediated Communication & 50 largest metropolitan areas in the U.S. Features include information about city, opportunities for citizen feedback, galleries of photos, links, etc.  Purely descriptive analysis, doesn't contain anything that isn't covered in any of the other aricles. & No \\ \hline
		\end{tabular}
		\label{}
	\end{table}
\end{landscape}

There doesn't seem to be much literature on transitions of party control, if anything, that question is mostly phrased with regard to political representation. However, if we want to tie our paper to a larger theory, we could go with dynamic representation. Under dynamic representation, policy-makers are responsive to trends in citizens opinions, which mainly manifest/become apparent through election results, especially when incumbents are voted out of office. This fits our topic quite well. Also, of the few papers that do exist on the effects of partisan transitions, virtually all use regression discontinuity designs.

\textbf{Why does the content of city websites matter?} According to \cite{Mayhew1974}, politicians engage in advertising, credit claiming and position taking in order to get re-elected. Official city websites allow mayors to do all three. Their offices frequently take a prominent position on the frontpage, and many websites also feature a picture of the candidate. In local politics, where campaign funds are low, this lends the incumbent a crucial advantage in becoming more well-known among her constituents. Furthermore, municipal politics gives incumbents clear and tangible achievements they can point to, such as completed infrastructure projects, the acquisition of federal or state funding, or the hosting of city-wide events. City websites present an opportunity for local officials to brandish these accomplishments. Finally, they also give mayors a platform from which they can advertise their political beliefs. On municipal websites, this may not manifest in the form of brazen partisanship, but more subtle avenues are available. As noted by \cite{Einstein2015a}, there are stark differences in the spending preferences of Democratic and Republican mayors. City websites can then be used to communicate the stance of a mayor on social or economic programs. Another advantage of websites with regard to communication is that unlike direct social interactions, officials have full control over them.

In addition to the use of city websites for the politicians that control them, variance in content also matters  with regard to the people who visit them. Local residents likely rely on city websites to get news about events, hot-button political issues specific to their city, contact city officials or find out addresses or opening hours of city institutions. Visitors use city websites to look up local attractions, which are often described in great detail. Similarly, prospective residents looking to move, might rely on city websites to inform their decision on whether to relocate there. An inviting website emphasizing the city's receptiveness to new residents might make a real difference here. Finally, city websites frequently feature sections on business, but there is a lot of variance in this area: Some emphasize economic development, properties, or transportation, whereas others focus on undeveloped land and other business opportunities. Differences in websites likely say something about a city's economic profile, with potential repercussions for the political realm.

\subsection*{Website Analysis}
The literature making use of scraped websites clusters into a number of categories. One, and most pertinent to our own endeavors, the e-governance literature which discusses the online presence of governments from a usability and public service point of view. For the most part, research in this category develops a classification scheme to rate websites in terms of accessibility, ease-of-use and function, and then hand-codes a set of websites according to these criteria \citep{Urban2002,Armstrong2011,Feeney2017}. As an example, \cite{grimmelikhuijsen2012developing} study local government websites with the goal of uncovering how they aid the goal of transparency. To this end, they analyze a set of Dutch municipalities in which air quality had deteriorated. The authors test whether local governments provide citizens with information about potential complications and solutions associated with this issue. Like most e-government studies however, this publication does not make any use of automated text analysis.

Websites have also played a major role in the field of media studies, as scholars have scraped and analyzed the online presence of newspapers, as well as the more diffuse world of online political blogs \citep{Adamic2005,Gentzkow2010}. \cite{Lin2011} provide a good example for a study which makes extensive use of automated content analysis - a necessity arising from its dataset of 66830 blog posts and 57221 online news articles. The authors estimate the political slant of these entities by counting the frequencies with which politicians of either side are mentioned and determine that blogs are generally more biased. Unfortunately for us, the authors don't go into the details of their text analysis, and offer no information on the acquisition and pre-processing of the data.

Another well-known example fitting into this area of study is the set of studies conducted by King et al. \citep{KING2013,King2014,KING2017}, in which the authors study censorship by the country's government on its lively blogosphere. However, the authors also provide no information on how their data was collected ``our extensive engineering effort, which we do not detail here for obvious reasons [...]''.

The websites of politicians and their parties have also fallen under scholarly scrutiny. Researchers have found that in order to identify the constituencies, motives and modes of communication of these actors, their websites can be very illuminating sources of information \citep{Druckman2009,Druckman2010,Cryer2017,Esterling2011,Esterling2011a,Norris2003,Therriault2010}. \cite{Druckman2009,Druckman2010} rely on the Mational Journal to find the websites, then hand-coded them. \cite{Cryer2017} provides fairly little information, but does mention the fact that she relied on Archive-it, the webservice of the Internet Archive we discussed recently. \cite{Esterling2011,Esterling2011a} rely on hand-coded data by the Congressional Management Foundation, a nonprofit organization which aims to assist Congress. \cite{Therriault2010} (a working paper) actually portends to use automated text analysis, and also has the most extensive overview of the associated methodology. However, the division of the website into sections (home page, topics, issues, details) is done by hand, and the actual analysis is incomplete. The author acquired the websites from the Library of Congress (which only collected them from legislators who actually consented, and Therriault notes that this causes nonrandom missingness).

Importantly for us, research analyzing and improving the scraping, pre-processing and analysis methods of this literature is scarce. \cite{Eschenfelder2002} provide something of an overview of how how federal websites should be assessed from an e-governance point of view, but they largely focus on the substantive criteria that should be fulfilled, rather than the technical aspects of website acquisition and analysis.

\section{Data}
The General Services Administration (GSA) maintains all .gov addresses, and provides a complete\footnote{Domains used for testing and internal programs are excluded.} list of all such domains to the public through GitHub\footnote{https://github.com/GSA/data/tree/gh-pages/dotgov-domains}. This list is updated once per month - we rely on the version released on January 16, 2017. The data from the GSA contains the following variables: One, domain name, specifically, the all-uppercase version of domain and top-level domain (for example, 'ABERDEENMD.GOV'). Two, the type of government entity to which the domain is registered, such as city, county, federal agency, etc. Three, for federal agencies, the name is specfied. Finally, the city in which the domain is registered, is noted.

Here, we focus only on cities. As a first step, we use a webdriver-controlled browser (Firefox/Selenium/Geckodriver) to test whether all of the city websites actually work. Of the 2425 domains listed by the GSA as cities, 292 are not accessible. Furthermore, the .gov domain, as registered at the GSA, is frequently not the website a city actually uses. In many cases, these sites redirect to another address, sometimes not a .gov domain (in this case, we simply use this domain). We record these URLs, as they are required to retrieve the images websites stored in the Wayback Machine (WbM).

$T_{1}+\beta_1$

In order to provide an overview of our coverage (as not all cities, towns and villages use .gov addresses), we merge this list with U.S. Census data\footnote{http://www2.census.gov/programs-surveys/popest/datasets/2010-2015/cities/totals/sub-est2015\_all.csv}. Here, several limitations in the GSA data need to be accounted for: One, even though the GSA nominally separates websites of cities and counties, some of the domains categorized as cities actually belong to counties. The same is true for townships and boroughs. Ergo, we eliminate all websites belonging to these three types of entities by hand. Furthermore, the city name, as given by the GSA, refers to the city in which the domain is registered, which is not necessarily equivalent to the city the website serves. In many cases, a website of a larger city may be registered to one of its subdivisions (for example, the website of New York is registered to Brooklyn), or vice versa (for example, the website of Homecroftin, a small town within Indianapolis, is registered to the city as a whole). Consequently we fix mismatches between websites and cities manually. Finally, a number of cities are simply misspelled, which we also correct by hand.

After the counties, townships and cities that cannot be matched to the Census data\footnote{There are five cities that are not contained in the Census data} and duplicate websites (some cities have more than one website) are removed, 1813 domains/cities remain.

These cities contain 90,616,865 people, and thus about 28\% of the U.S. population (see figure 1).

\begin{figure}[!ht]
	\centering
	\caption{Percentage of state population covered.}
	\includegraphics[width=0.9\linewidth,height=0.9\textheight]{figures/coverage_states.pdf}
\end{figure}

We use the resulting list of websites to acccess their copies stored in the Internet Archive's Wayback Machine. To this end, we rely on the Ruby Gem 'Wayback Machine Downloader'\footnote{https://github.com/hartator/wayback-machine-downloader} (WbMD). We supply the URL that each .gov website redirects to to the WbMD, which then downloads every file present in the WbM from a snapshot in October 2016, or, if not available, as soon as possible after this point.

<Note: We have not actually done this last step for all websites (however, the R script which runs the Ruby package is already set up to do so once we need to). Instead 10 websites were randomly sampled from an older version of the GSA list, which still contained counties and townships, which is why one of the 10 websites is from Dutchess County, NY.>

\input{tables/filetypes.tex}

\begin{landscape}
\input{tables/filenumbers.tex}
\end{landscape}

For some cities, whose websites make heavy use of JavaScript, this method does not lead to satisfying results. Consequently we restricted our corpus to cities with at least 3 documents.

\section{Preprocessing}

The documents are read in line by line, converted to UTF-8 and then stripped of dates, punctuation, numbers and words connected by underscores. At this point, the documents of one city still closely resemble one another in the form of boilerplate content, be it website elements (i.e. "You are here", "Home", "Directory" etc.) in html documents, or commonly used forms or phrases in pdfs, doc and docx files. This is an issue, because it clusters documents around the cities from which they originate in a way that has nothing to do with their actual content. In other words, the signal would be drowned out by the noise. Consequently we remove this content as following: Each line of every document is compared to every line in every other document belonging to the same city. We count how many times each line is duplicated for that city. We remove any line occuring more than our chosen threshold of 10.\footnote{Empirically, lines tend to be duplicated either hundreds of times, or only once or twice, if at all.} This means that each document only retains the information that is particular about it.
Preprocessing further includes setting every character to lowercase, as well as the removal of bullet points which frequently occur in html documents, extraneous whitespace, xml documents mislabeled as html files, and empty documents. Furthermore, some documents contain gibberish, often as a result of faulty or impartial OCR. To combat this problem, we employ two solutions. One, we use spellchecking, implemented through the hunspell R package, to remove all non-English words. However, hunspell does not cover everything, either because some tokens are not actual words (for example artifacts from defective encoding), or because random sequences of characters just so happen to form words that exist in a dictionary (for example "eh" or "duh"). Since we rely on a bag-of-words model in which syntax does not matter, we can ameliorate these problems by removing all text except for whitespaces and the characters that appear in the english alphabet. Since a lot of the nonsensical text tends to be quite repetitive, we also delete all documents in which the proportion of unique to total number of tokens is less than 0.15. Furthermore, hunspell does not spellcheck individual characters, so we remove all individual characters appearing as tokens except for "i" and "a". Since these pre-processing steps reduce documents which are largely unsuitable to only a few words of texts that don't make much sense, we also remove all remaining documents containing less than 50 tokens.

\subsection{Indiana City Websites}

It would be fine to focus on Indiana as a case. First, we need to answer some preliminary questions about the data.

\begin{enumerate}

\item For what percentage and number of IN cities can we find data from the WBM?
\item For how many election cycles can we find political leadership data for these matched cities?
\item In what number and percentage of cities is the local leadership majority Republican? 
\item Relatedly, in a typical election cycle, for how many cities do we see a transition in party leadership (i.e., a shift from majority D (R) to majority (R) D). 

\end{enumerate}

\begin{enumerate}
	
	\item 30 cities, with a combined population of 1,180,435. However, since only cities (as opposed to towns and villages) hold mayoral elections, only 16 of these, with a combined population of 1,094,383 can be matched to the election data.
	\item 2015, 2011, 2007, 2003.
	\item Of the 16 cities, 7 have Republican mayors after the 2015 elections.
	\item In 6 cases, a shift of party control occurs, with 4 of these being Republican --> Democratic. 
	
\end{enumerate}

% latex table generated in R 3.3.3 by xtable 1.8-2 package
% Wed Mar 22 11:32:43 2017
%\begin{table}[ht]
%	\centering
%	\begin{tabular}{lrrlrrl}
%		\hline
%		City & DemVotes & RepVotes & Winner & Change & Pop15 & url \\ 
%		\hline
%		Attica &  & 187 & Republican & 0 & 3117 & https://attica-in.gov/ \\ 
%		Connersville & 1005 & 995 & Democratic & 1 & 13010 & http://connersvillecommunity.com/ \\ 
%		Frankfort &  & 1748 & Republican & 0 & 16060 & http://frankfort-in.gov/ \\ 
%		Huntingburg & 447 & 793 & Republican & 0 & 6035 & http://www.huntingburg-in.gov/ \\ 
%		Indianapolis & 92830 & 56661 & Democratic & 1 & 862781 & http://www.indy.gov \\ 
%		Lake Station & 1483 & 227 & Democratic & 0 & 12054 & http://www.lakestation-in.gov/ \\ 
%		Linton & 785 & 692 & Democratic & 0 & 5284 & http://www.linton-in.gov/ \\ 
%		Madison & 1192 & 1915 & Republican & 0 & 12040 & http://www.madison-in.gov/ \\ 
%		Mitchell & 229 & 495 & Republican & 1 & 4252 & http://mitchell-in.com/ \\ 
%		Monticello & 0 &  & Democratic & 0 & 5322 & http://www.monticelloin.gov/ \\ 
%		North Vernon & 679 & 697 & Republican & 1 & 6619 & http://www.northvernon-in.gov/ \\ 
%		Richmond & 3421 & 2731 & Democratic & 0 & 35854 & http://www.richmondindiana.gov/ \\ 
%		Rockport & 286 & 272 & Democratic & 1 & 2223 & http://www.cityofrockport-in.gov/ \\ 
%		South Bend & 8515 & 2074 & Democratic & 0 & 101516 & https://www.southbendin.gov/ \\ 
%		Union City & 338 & 440 & Republican & 0 & 3447 & http://www.unioncity-in.gov/ \\ 
%		Winchester & 606 & 524 & Democratic & 1 & 4769 & http://www.winchester-in.gov/ \\ 
%		\hline
%	\end{tabular}
%	\caption{} 
%\end{table}

\subsection{Research Design}

\begin{table}[ht]
	\centering
	\begin{tabular}{llr}
		\hline
		Variable & Unit & Source \\
		\hline
		Population size & 1000 people & Census \\
		Population growth last 5 years & Percent & Census \\
		Type of economy (agriculture/industry/services) & ? & Census \\
		Economic performance (GDP?) & \$ & Census \\
		Party of mayor before election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
		Party of mayor after election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
		Change of party control & 0/1 & in.gov/sos/elections/ \\
		Presidential vote 2012 in county & Percent Rep & ? (but I have the data) \\
		Unemployment rate & Percent & Census \\
		Broadband speed & Avg. Mbps DL & broadbandmap.gov \\
		\hline
	\end{tabular}
	\caption{List of covariates} 
\end{table}



\begin{enumerate}
\item Corpus:
\begin{enumerate}
\item Last snapshots before the election (November 3, 2015 in Indiana; tbd. in Louisiana (probably February))
\item First snapshot that is at least 2 months after the new government's inauguration (which is in January for Indiana, May for Louisiana)
\end{enumerate}
\item Preprocessing:
\begin{enumerate}
\item restrict corpus to:
\begin{enumerate}
\item documents belonging to cities in which a change of power occurred
\item documents that were added, deleted or changed between the two snapshots
\end{enumerate}
\item words to lowercase
\item remove punctuation
\item stemming (Porter stemming algorithm?)
\item Remove stop words (regular list of stop words is enough, since we use an asymmetric prior)
\end{enumerate}
\item Apply Grimmer's expressed agenda model to the corpus
\begin{enumerate}
\item Asymmetric prior
\item Each document can have only one topic (in contrast to the author-topic model)
\item Cities $i = 1,..., n = 15$
\item Topic $k(k = 1,..., K )$
\item Documents $j(j = 1,...,D_i)$ from city i
\item Party covariate in the prior, where the deleted and unmodified documents are coded as from the first, and the added and modified documents from the second party
\end{enumerate}
\item Results
\begin{enumerate}
\item Label topics using Grimmer's automatic cluster labeling method, based on most commonly used words in documents belonging to topic
\item Evaluate topics
\end{enumerate}
\end{enumerate}

Validation:

\begin{itemize}
\item Do the above for cities in which no change of power occurred.
\item Check whether there is higher than average turnover around the new year by comparing changes to non-election years (and also Louisiana, where elections are later).
\item Check how long documents stay on websites on average. Use websites with a lot of snapshots for this (these exist for both small and large cities).
\end{itemize}

Problem with using this model: Grimmer's expressed agenda model uses Senators as the actors. Senators is also who he is substantively interested in. For us, the equivalent to Senators is cities. However, we care about parties, not cities.

\subsection{Survival model}
The existence of individual documents on municipal government websites can be though of as a survival process. No document stays on a website forever, and it appears to be a reasonable assumption that as documents get older and thus less relevant, they get replaced. The factors determining the steepness of the survival curve are the topic - fire safety regulations likely stay up longer than a bulletin on the annual spring banquet - and the change of party control after an election.

\begin{quote}
\textit{H1}: The older a document, the more likely it is to be removed.
\end{quote}

$S(t)$ has a downward slope. Admittedly, this is almost impossible not to be true. Also, test proportional, rising and falling hazard models.

\begin{quote}
\textit{H2}: Documents pertaining to administrative matters are less likely to be removed.
\end{quote}

Introduce a categorical variable for the top 10(?) topics. A negative coefficient for administrative topics would support this hypothesis.

\begin{quote}
\textit{H3}: Documents introduced by the opposing party are more likely to be removed.
\end{quote}

Introduce two variables into the survival model: One variable indicating which party has introduced a document, and a time-varying variable describing which party is currently in government. The hypothesis is tested through an interaction term between the two.

\begin{quote}
\textit{H4a}: Democrats are more likely to remove documents with topics pertaining to private enterprise, private schools.
\end{quote}

Interaction term between party in power and categorical topic variable.

\begin{quote}
\textit{H4b}: Republicans are more likely to remove documents with topics pertaining to social justice, equality, taxes, public schools, etc.
\end{quote}

Interaction term between party in power and categorical topic variable.

\begin{quote}
\textit{H5}: In line with their commitment to small government, Republicans are more likely than Democrats to remove documents.
\end{quote}

Party in power variable.\\


This model will take up a lot of degrees of freedom. The rarity of snapshots for some cities might be a problem. Documents being changed and being removed can be modeled as competing risks.


\begin{equation} 
\label{eq1}
\begin{split}
Y & = \text{Party that introduced the document} \\
 & + \text{Party that is currently in power} \\
 & + \text{Topic 1, topic 2, ..., topic k} \\
 & + \text{Party that is currently in power} \times \text{Topic 1, topic 2, ..., topic k} \\
 & + \text{Days since start of mayoral term (control)}
\end{split}
\end{equation}

\subsection{Topic modeling}
Note: this chapter is mostly a wordy and less coherent version of the above.

We hypothesize that a change in leadership from one party to the other will lead to a change in website content because the two parties have different agendas. Democrats have a predilection towards policies that promote social and economic equality, whereas Republicans like to emphasize small government as well as law and order. Documents uploaded to city websites are expected to be a reflection of these preferences.

The Latent Dirichlet Allocation (LDA) \citep{Blei2003} is the most commonly used topic model. However, it is unable to account for the existence of two parties with very different policy agendas, translating to different preferred topics. There are two types of extensions to the LDA that fit our subject much better - the structural topic model, and the author-topic model.

The structural topic model, developed by \cite{Roberts2016a} allows researchers to model a corpus as a function of metadata associated with its documents. Specifically, topic prevalence (the proportion of a document made up by a topic) and topical content (the rate at which words figure into a topic) are contingent on a set of covariates. In our case, the two most important covariates are (1) city and (2) authoring party (operationalized by whether a document was present before a change of power, or introduced afterwards). Furthermore, the population size of the city should be a predictor for both the number and kind of problems it faces, which thus need to be addressed on its website. Furthermore, city size also serves as an estimate for the budget and technical capacities of its staff in charge of maintaining the website\footnote{Although this relationship is not exactly deterministic - when looking through .gov websites manually, I've noticed that a lot of websites of (presumably wealthy)  towns of only a few thousand citizens often have extremely well-kept websites}.  Further demographic as well as economic data might also be useful to differentiate cities from one another. If we model the differences between cities properly, we might not have to/should not include city as a (categorical) variable, because it would probably interfere with these more meaningful covariates.

The author-topic model \citep{Rosen-Zvi2004} would allow us to capture the fact that different authors have different topical preferences. Unfortunately, we have two types of 'authors' - cities, and parties. Given the largely divergent administrative needs of different types of cities, we would likely have to treat cities as the author. This would require us to capture the partisan authorship of different documents entirely on the basis of sub-sets of the website data - changed, added, or deleted documents. (Note: In the papers on author-topic models, the intention is often to analyze scientific articles. These articles are often co-authored. Would it be possible to have BOTH cities and parties as authors, so that a specific version of a website would then be 'co-authored' by its city and party?)

The critical element in this analysis is to accurately attribute authorship of documents to either party. Despite possible changes to websites due to a leadership transition, large parts of the content carry over. This means that unless the successor government decides to delete everything, some of the existing documents will be preserved, and in the model, also attributed to the new 'author'. But the reverse is not possible, because the predecessor government can't choose to retain documents from the future. {\em  This is a very important point for municipal websites. We should investigate the possibility of modeling only the changes---documents that change, documents that are deleted, and documents that are added. }

Labeling newly added documents after a change of power is quite simple. As far as older documents are concerned, we would have to operate under the assumption that the incumbent didn't keep his or her successor's document's on the website for four years.\footnote{Probably a safe assumption. However, we could, and probably should test how long documents tend to stay on a city website. Simple descriptive statistics (for example density plots) on the length of existance should likely be sufficient. If we want to be really fancy about it, we could create a duration model, with document topics as features. This would allow us to measure whether some documents tend to remain longer based on their topic (i.e. fire regulations are probably going to stay up longer than notes on a specific council meeting).} One problem here is the fact that the incumbent would have all the administrative topics assigned to them, simply because they have to have those on their website.

If we really do end up getting swamped with administrative terms in our topic models (and it does kind of look like that at the moment), we might be able to separate the signal from the noise by running a preparatory LDA once and using its results to create a new, corpus-specific list of stop words. After that, we run the actual model. This way, politically charged terms and topics, which likely are not as common, but present nevertheless, should be able to rise to the surface. It might be possible to refine this process by running an exploratory model on website data from cities in which party control never changes, and the incumbent always wins by large margins. 'Safe' cities like this should have fairly homogeneous populations, with little need for the incumbent to play politics on the municipal website. Hence, these websites should be filled with purely administrative content.

The use of asymmetric priors \citep{Wallach2009} over the document topic distribution - i.e. the assumption that some topics, such as administrative content, are inherently more common - may be a more elegant way of dealing with this issue.

Another intervening factor is that for cities in Indiana, mayoral terms begin in January. Since a lot of clerical and administrative tasks tend to be year-specific, work tends to pile up around the new year. Thus it is possible that a spike in newly added documents is not due to a change in party control, but owed to a seasonal increase in activity. We can test for this by comparing election years to non-election years. Furthermore, since in Louisiana, mayors take office in May, we have another point of comparison.

Furthermore, if we only investigate cities in which control of government changes from one party to another, we may overestimate its effect. Not only does a transition in party control occur, but the person in charge also changes. Parties are fairly homogeneous, so that two mayors from the same party may have very different policy preferences and managerial styles. To remedy this problem, we [could] utilize matching, pairing our cases with similar cities in which the incumbent does not run for re-election, but party control stays the same nevertheless.

%Justicifcation for Arun 2010 topic number measure
In order to determine the optimal number of topics for our corpus, we utilize the measure developed by \cite{Arun2010}. In contrast to other approaches such as \cite{Cao2009} or \cite{Griffith2004}, this measure includes as much information about the model as possible by taking into account both the document-term matrix, as well as the word-topic matrix. We find 200 topics to be the best value (see figure \ref{ldatuning}).

%One solution might be to rely on LDA models, but only to apply them to documents that were deleted, the strongest possible expression of a successor governments diverging policy preferences. This way, we could identify directly which issues are considered to be the most divisive.

%There is also a possibility that city size is a determinant of the number of topics. Larger cities tend to have bigger websites, owing to a larger set of issues that need to be dealt with.

%Traditional author models that rely on lexical or syntactic differences may also be of use. Parties and their voters are associated with specific educational backgrounds, which may in turn be associated with specific writing styles. However, it seems likely that personal rather than partisan authorship is playing an even larger role here.

%another feasible way of modeling the way in which websites receive a makeover when party control of the mayorship changes. As noted by \citep{Rosen-Zvi2004}, LDA can be considered a special case of the author-topic model, ''where each document has one unique author''. Here, a city website would be a collection of documents, each of which originates entirely from either Democratic or Republican leadership. In a way, this captures the data-generating process more closely.

%Even so, neither the author-topic model nor the LDA model are a perfect fit for the structure of city government websites. One assumes that multiple authors write one document. This may be appropriate for the original purpose of author-topic models, scientific papers, which have multiple authors. This is not as good of a fit for city governments however. Using the LDA instead deals with this problem, but it also assumes that one document has multiple topics. For city government websites, this may be an erroneous assumption, because in many cases, each of these files appears to have one very specific purpose - agricultural, parks and recreation, smoke detectors, and so on.+

%\footnote{This may be wrong in some cases, as they might be leftovers from yet another predecessor government. If the WaybackMachine data reaches back far enough, we might be able to identify and include only the documents that were added since the previous election.}, and the new documents as stemming from party 2. The number of topics \textit{j} would be set to at least three, where one topic would (hopefully) be reserved for purely administrative, non-partisan issues so that the other two (or more) would resolve to party-specific policy issues. Ideally, the result would then show significant differences in the probabilities of authors being assigned to partisan topics.

\begin{figure}[!ht]
	\centering
	\caption{Dates of Wayback Machine snapshots. The vertical lines are municipal elections.}
	\label{snapshots}
	\includegraphics[width=\linewidth]{figures/Snapshots.pdf}
\end{figure}


\begin{figure}[!ht]
	\centering
	%\caption{Population coverage}
	\includegraphics[width=\linewidth]{figures/wtp_current.pdf}
\end{figure}

\begin{figure}[!ht]
	\centering
	%\caption{Population coverage}
	\includegraphics[width=\linewidth]{figures/wtp_current_dem_rep.pdf}
\end{figure}

%\begin{figure}[!ht]
%	\centering
	%\caption{Population coverage}
%	\includegraphics[width=\linewidth]{figures/wtp_before.pdf}
%\end{figure}

%\begin{figure}[!ht]
%	\centering
	%\caption{Population coverage}
%	\includegraphics[width=\linewidth]{figures/wtp_after.pdf}
%\end{figure}

\begin{figure}[!ht]
	\centering
	%\caption{Population coverage}
	\includegraphics[width=\linewidth]{figures/wtp_current_dem_rep_order.pdf}
\end{figure}

%topicweights density
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Densities of topic weights for documents in Republican and Democratic cities.}
	\label{doctopics_density}
	\includegraphics[width=\linewidth]{figures/doctopics_density.pdf}
\end{figure}

\subsubsection{LDA diagnostics}

Figure \ref{doctopics_density} plots the densities of topic weights across documents, where each line represents a topic. Distributions with lower peaks near zero and flatter bodies indicate a topic that is present to varying degrees in multiple documents. This shape appears to be more common in documents from Republican cities. By contrast, a distribution with a high peak near zero and a very long tail is indicative of a topic that only appears in one specific document. This appears to be more common for documents in Democratic cities. The substantive conclusion then is that Democrats appear to be more prone to dedicating individual documents to one specific purpose, whereas Republicans produce general-purpose documents more often.

%topicweights density
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Densities of topic weights for documents in Republican and Democratic cities.}
	\label{topicweights_density}
	\includegraphics[width=\linewidth]{figures/topicweights_density.pdf}
\end{figure}

An alternative way to consider topic weights is to aggregate them across documents, in this case through their median or mean. Figure \ref{topicweights_density} shows this distribution of topic weights across topics. Most topics have a very low weight - meaning that they do not appear in a lot of documents. When aggregating across documents via the mean, this effect is more pronounced for Republicans, suggesting that most topics do not feature frequently (or even at all) in their documents. Democrats on the other hand appear to have a wider spectrum of topics from which they chose - supporting the hypothesis of Democrats as a 'big tent' party. However, this effect is reversed when using the median instead of the mean to aggregate across documents. Now, the the distribution has a higher peak and lower tail for Democrats. The cause for this stark divergence appears to lie in the fact the mean and median differ enormously - for the range displayed here, the mean is about a hundred times greater. Evidently, extremely large weights (i.e. some documents fitting specific topics perfectly) distort the picture. The median document is a better representation of the corpus as a whole, but the results do not fit our hypothesis. Their are however more consistent with figure \ref{topicweights_density}.


Topics for which there are stark differences in their distribution across documents between Democrats and Republicans are of particular interest to us. To detect these topics, we take the absolute difference of the median document topic weights, and arrange them as a histogram, see figure \ref{diffsize}. For the most part, differences are small. For a few topics however, a contrasts emerges.

%size of the differences
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Word-topic probabilities for topics with big partisan differences}
	\label{diffsize}
	\includegraphics[width=\linewidth]{figures/diffsize.pdf}
\end{figure}

Figure \ref{plotWTP_bigdiffs} displays the word-topic probabilities for the 10 topics with the largest partisan differences. Some of these, such as topic 82, with its focus on policing, safety and crime could be construed as politically charged. For the most part however, these topics focus on administrative matters.

%word topic probabilities for topics with big differences
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Word-topic probabilities for topics with big partisan differences}
	\label{plotWTP_bigdiffs}
	\includegraphics[width=\linewidth]{figures/plotWTP_bigdiffs.pdf}
\end{figure}

To further investigate the topics with large partisan differences, figure \ref{plotWTP_bigdiffs} shows densities of topic weights just like figure \ref{doctopics_density}, but reduced to only the topics with the largest differences. It appears that mere absolute differences between the medians (or some other measure of central tendency) obscures the fact that the distributions themselves are quite different.

%word topic probabilities for topics with big differences
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Word-topic probabilities for topics with big partisan differences}
	\label{doctopics_density_bigdiffs}
	\includegraphics[width=\linewidth]{figures/doctopics_density_bigdiffs.pdf}
\end{figure}

To further investigate this issue, we look at the raw data itself - the document-topic matrices for Democrats and Republicans, displayed as heatmaps. Figure \ref{heatmaps_weights} shows that especially for Republicans (but also for Democrats, to a lesser extent), there appears to be extensive clustering for consecutive documents. Since the order of the documents in the corpus is dependent on the cities in which they appear in, it seems that topics are mere representation of city websites - each website `owns' a number of topics, that appear across all of its documents, and hardly anywhere else. One possible cause for this type of clustering is the fact that documents frequently share common words, for example pertaining to navigation on the site, or standard forms that are shared throughout all documents.

\subsubsection{Structural topic model}
An ostensibly intuitive solution to topics clustering into cities in LDA is to include dummies for the cities in a statistical model of topics. This is facilitated by the structural topic model, which uses metadata on the document to account for variation in topics \citep{Roberts2014}. However, figure \ref{stm_results} shows that if anything, the STM exacerbates the problem. Here, we plot the \textit{p-values} of the coefficients for each city as well as the party variable across each topic. Under normal circumstances, plotting the p-values, as opposed to the fitted values, does not make much sense, but here it serves a diagnostic purpose. The plot shows that the party variable is never statistically significant at any conceivable level of confidence, nor is it even close to. Interestingly the same is true for a number of the cities as well. The topics cluster heavily into only about half of the cities, which does not present an improvement over LDA at all.

\subsubsection{Prediction with SVM}
An alternative approach to the problem is to ignore topics entirely, and go straight to predicting documents that are much more likely to be included on websites belonging to one or the other party. Classic machine learning techniques such as Naive Bayes, Linear Discriminant Analysis, or SVM should be expected to fare well in this context. Here, we rely on SVM, implemented with the SciKitLearn package in Python.\footnote{We also implemented SVM in R through the packages kernlab and e1071 in R. However, neither of these provide a regularized version of SVM (NOTE: at least that is what I am gathering from the stack overflow error), which prevents us from using all of the features contained in our data. Instead, we ranked the features according to tf-idf and selected the top 5000. These methods are also quite slow, and provide a maximum accuracy of 82\% in five-fold cross validation.} A grid search reveals the tf-idf representation of the document-term matrix to be better than pure word counts, unigrams to be superior to bi-grams, the application of an L2 penalty to be preferable to either L1 or elasticnet, and an alpha (a constant to multiply with the regularization parameter C) of 0.0005 to lead to the best results. Applying five-fold cross-validation to the (tf-idf) document-term matrix with the dimensions 16011x35000 leads to an average accuracy of 89\%.\footnote{Other methods used: Elastic-net in the glmnet package in R. Accuracy is 0.6924795 for in-sample prediction, so not worth bothering with.}

However, \cite{Monroe2008} advise against using these types of methods in this context because they get the data generation process backwards: Our theory assumes that party leads to variation in writing, and yet we rely on the documents to predict party, in spite of the fact that we actually have perfect knowledge of it.

\subsubsection{Informative Dirichlet model}
Consequently we follow their recommendation to use the informative Dirichlet model they present in the paper \citep{Monroe2008}. This approach aims to account for the fact that some words naturally occur more than others by applying a Dirichlet prior based on the distribution of words in random text. Figure \ref{fightinwords} shows the top words for both Democrats and Republicans - and accomplishes, to some extent, the goal of \citep{Monroe2008} of banishing frequent words from this list and supplanting them by text with greater semantic, and in our case, partisan meaning. For Democrats, words related to public finances, such as 'fund', 'budget', or 'tax' are common - congruent with the party's greater willingness to raise and spend money publicly. Similarly, 'federal' 'funds' also appear among the Democratic top words. By contrast Republicans prefer to devote their attentions to purely local, and for the most part, logistic matters. 'Trees', 'water', 'street' and 'sign' appear on the list of Republican top words, suggesting that these mayors largely focus on city planning.

These top words also show some degree of overlap with those determined by aggregating over topics in LDA.

%Heatmaps
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Word-topic probabilities for topics with big partisan differences}
	\label{heatmaps_weights}
	\includegraphics[width=\linewidth]{figures/heatmaps_weights.png}
\end{figure}

%Number of topics vs. tophic coherence
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Topic coherence, varying the number of topics. The red line represents the mean topic coherence for each number of topics.}
	\label{numtopics_coherence}
	\includegraphics[width=\linewidth]{figures/coherence_numberoftopics.pdf}
\end{figure}

%Indiana map
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Cities in the corpus, by partisanship of mayor.}
	\label{indiana_map}
	\includegraphics[height=0.75\textheight]{figures/indiana_map.pdf}
\end{figure}

%partisan topics
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{The most partisan topics. The number in parenthesis indicates how many times more the topic appears on average (measured through the number of words of the topic throughout the documents) in the respective party's corpus (indicated by the color).}
	\label{partisan_topics}
	\includegraphics[height=0.75\textheight]{figures/partisanTopics.pdf}
\end{figure}

%ldatuning
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{The optimal number of topics for our corpus. The measures of Griffths 2004 and Cao 2009 indicate the best number of topics at their minimum, whereas the measure of Arun 2010 points to the best value at its maximum.}
	\label{ldatuning}
	\includegraphics[height=0.75\textheight]{figures/topicnumber_ldatuning.pdf}
\end{figure}

%stm results
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Results from a structural topic model, displayed as the p-values for each variable for each topic. This would normally be somewhat nonsensical, but here it illustrates why the model does not work.}
	\label{stm_results}
%	\includegraphics[width=1.1\linewidth]{figures/stm_results.pdf}
\end{figure}

%fightin words results
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Top democratic and Republican words (Indiana), according to the informed Dirichlet model of Monroe et al. (2008). Ordering is top to bottom for Democrats, and vice versa for Republicans.}
	\label{fightinwords}
	\includegraphics[width=\linewidth]{figures/fightinWords.png}
\end{figure}

%fightin words results
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Top democratic and Republican words (Louisiana), according to the informed Dirichlet model of Monroe et al. (2008). Ordering is top to bottom for Democrats, and vice versa for Republicans.}
	\label{fightinwordsLA}
	\includegraphics[width=\linewidth]{figures/fightinWordsLA.png}
\end{figure}

%Partisan top words - topic model Indiana
\input{tables/partisanTopWords.tex} %\ref{tabFightinIN}

%Partisan top words - topic model Louisiana
\input{tables/partisanTopWordsLA.tex} %tabLDALA

%Partisan top words - fightin words Indiana
\input{tables/fightinwordsIN.tex} %\ref{tabFightinIN}

%Partisan top words - fightin words Louisiana
\input{tables/fightinwordsLA.tex} %\ref{tabFightinLA}

%Partisan top words - stm Indiana
\input{tables/stmTopWordsIN.tex} %\ref{tabSTMLA}

%Partisan top words - stm Louisiana
\input{tables/stmTopWordsLA.tex} %\ref{tabSTMLA}

%Some basic descriptive statistics of documents by party
\input{tables/descriptiveStatisticsPartisanIN.tex}

%Some basic descriptive statistics of documents by party
\input{tables/descriptiveStatisticsPartisanLA.tex}

%fightin words results
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Total number of lines retained at a given threshold for removing duplicated lines. For example, at x = 10, all lines occurring more than 10 times within a city's documents are removed.}
	\label{linesCutoff}
	\includegraphics[width=\linewidth]{figures/linesCutoffIN.pdf}
\end{figure}

%Clustering
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Hierarchical clustering (Indiana).}
	\label{hclustIN}
	\includegraphics[width=\linewidth]{figures/clusteringIN.png}
\end{figure}

%Clustering
\begin{figure}[!ht]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Hierarchical clustering (Louisiana).}
	\label{hclustLA}
	\includegraphics[width=\linewidth]{figures/clusteringLA.png}
\end{figure}

\newpage

\bibliographystyle{apsr} % apsr stopped working for me
%\bibliographystyle{plainnat}
\bibliography{ref}

\end{document}

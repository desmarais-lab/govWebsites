\documentclass[11pt]{article}

%==============Packages & Commands==============
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{tikz}
%%%<
\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{tikzpicture}
%\setlength\PreviewBorder{5pt}%

\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
% \geometry{a4paper}                   		% ... or a4paper or a5paper or ...
%\geometry{landscape}                		% Activat\usetikzlibrary{arrows}e for for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{arrows}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage[longnamesfirst]{natbib} % For references
\bibpunct{(}{)}{;}{a}{}{,} % Reference punctuation
\usepackage{changepage}
\usepackage{setspace}
\usepackage{booktabs} % For tables
\usepackage{rotating} % For sideways tables/figures
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{comment}
\usepackage{pgf}
\usepackage{xcolor, colortbl}
\usepackage{array}

\def\mybar#1{%%
	#1 & {\color{red}\pgfmathsetlengthmacro\x{#1*0.006mm}\rule{\x}{4pt}}}

%\pgfmathsetlengthmacro\x{#1^0.1mm}
%\show\x -> 25.0pt

%\usepackage{fullwidth}
\newcolumntype{d}[1]{D{.}{\cdot}{#1}}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{3}{D{.}{.}{3}}
\newcolumntype{4}{D{.}{.}{4}}
\newcolumntype{5}{D{.}{.}{5}}
\usepackage{float}
\usepackage[hyphens]{url}
%\usepackage[margin = 1.25in]{geometry}
%\usepackage[nolists,figuresfirst]{endfloat} % Figures and tables at the end
\usepackage{subfig}
\captionsetup[subfloat]{position = top, font = normalsize} % For sub-figure captions
\usepackage{fancyhdr}
%\makeatletter
%\def\url@leostyle{%
%  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
%\makeatother
%% Now actually use the newly defined style.
\urlstyle{same}
\usepackage{times}

\usepackage{lscape}
% \usepackage{mathptmx}
%\usepackage[colorlinks = true,
%						bookmarksopen = true,
%						pagebackref = true,
%						linkcolor = black,
%						citecolor = black,
% 					urlcolor = black]{hyperref}
%\usepackage[all]{hypcap}
%\urlstyle{same}
\newcommand{\fnote}[1]{\footnote{\normalsize{#1}}} % 12 pt, double spaced footnotes
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\def\citeaposs#1{\citeauthor{#1}' (\citeyear{#1})}
\newcommand{\bm}[1]{\boldsymbol{#1}} %makes bold math symbols easier
\newcommand{\R}{\textsf{R}\space} %R in textsf font
\newcommand{\netinf}{\texttt{NetInf}\space} %R in textsf font
\newcommand{\iid}{i.i.d} %shorthand for iid
\newcommand{\cites}{{\bf \textcolor{red}{CITES}}} %shorthand for iid
%\usepackage[compact]{titlesec}
%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\bibsep}{2pt}
%\renewcommand{\headrulewidth}{0pt}

%\renewcommand{\figureplace}{ % This places [Insert Table X here] and [Insert Figure Y here] in the text
%\begin{center}
%[Insert \figurename~\thepostfig\ here]
%\end{center}}
%\renewcommand{\tableplace}{%
%\begin{center}
%[Insert \tablename~\theposttbl\ here]
%\end{center}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Y}{\bm{\mathcal{Y}}}
\newcommand{\bZ}{\bm{Z}}

\usepackage[colorlinks = TRUE, urlcolor = black, linkcolor = black, citecolor = black, pdfstartview = FitV]{hyperref}


%============Article Title, Authors==================
\title{\vspace{-2cm} Government websites as data: A methodological pipeline with application to the websites of municipalities in the United States}


\author{ Markus Neumann \and Fridolin Linder \and Bruce Desmarais} \date{\today}



%===================Startup=======================
\begin{document}
\maketitle



%=============Abstract & Keywords==================

\begin{abstract}

A local government's website is arguably the most important general source of information about policies and procedures for residents and other community stakeholders. Accordingly, government websites have become prominent sources of data for a variety of research agendas in public administration, public policy, and political science. Existing research has relied on manual methods of website data collection and processing. However, reliance on manual collection and processing limits the scale and scope of website content analysis. We develop a methodological pipeline that researchers can follow in order to gather, process, and analyze website content with established text analysis techniques. First, for the acquisition of website data, we cover approaches to automated scraping methods. Second, pre-processing is a particularly vital step in text analysis, but when websites are concerned, additional measures need to be taken in order to guard against potential sources of bias. We propose a new method for dealing with the types of duplicated and boilerplate contents that are commonly found in government websites. We illustrate our methodological pipeline through the collection and analysis of a new and innovative dataset---the websites of over two hundred municipal governments in the United States. We build upon recent research that analyzes how variation in the partisan control of government relates to content made available on the government's website. Using a structural topic model to analyze municipal website contents, we find that websites of cities with Democratic mayors include more information about policy deliberation and crime control, whereas websites from cities with Republican mayors include more information about the provision of basic utilities and services such as water, electricity, garbage removal and fire safety; and information on municipal ordinances and policies.
%\noindent We explore the effect of transitions of power in municipal governments on the content of their websites. We hypothesize that when party control changes, city administrators modify the contents of their websites in order to fit the agenda of the new incumbent. To test this theory, we study cities in Indiana and Louisiana, two states in which all municipal elections are partisan and the parties of the candidates appear on the ballots. Snapshots of websites before and after transitions of power are acquired through the Wayback Machine. We apply statistical topic models based in latent dirichlet allocation, focusing on changes to the websites. We present results on both which topics see the greatest degree of change associated with transitions in city administrations, and how the topics modified differ with regard to political parties.

\end{abstract}
\thispagestyle{empty}
% \doublespacing
% Description of the possible challenges
\doublespacing
\section{Introduction}

Local governments convey voluminous information about all aspects of their policymaking, policy implementation, and public deliberation, via their official websites. The vital role of official websites in connecting the government and the governed has motivated a wave of research on the contents of government websites \citep[e.g.,][]{grimmelikhuijsen2010transparency,wang2005evaluating,osman2014cobra}. Despite the potential for automated scraping of website contents, the conventional approach to data collection in projects focused on government websites involves manual content extraction from each website in the dataset. Though highly accurate, the manual approach to data collection is costly, and cannot be scaled to capture even a fraction of the volume of content available on government websites. In this paper we present a methodological pipeline that can be used to automatically scrape government websites in order to build datasets that can be used for text analysis. We provide an illustrative application in which we explore the ways in which the textual contents on city government websites in six American states (IN, LA, NY, WA, CA and TX) correlate with the partisanship of the city mayor.

Though there exists a variety of software tools that are designed to automatically scrape all of the files available at a website \citep{glez2013web}, raw website downloads have to be processed significantly before the files are adequately prepared for text analysis. We describe and provide solutions to two central challenges in automatically gathering and analyzing website textual contents. First, plain text must be extracted from the files. This involves purging the files of syntax in HTML and other programming languages, and discarding any other character encoding errors that result from reading the files. This challenge would arise in any context in which researchers sought to study the textual contents of websites, and is not unique to comparative analysis of government websites. The second challenge we address in our methodological pipeline is, however, specific to the research objective of comparing websites on the basis of a common lexicon. For any two governments, the textual signatures that most dramatically differentiate the textual contents of their websites consist of what we can call ``boilerplate'' text---header, footer, or other titling text that is designed to identify the website as being associated with a specific government entity (e.g., ``Welcome to the city of Santa Cruz'', ``The City of Los Angeles welcomes you''). This boilerplate text is replicated across many files that are associated with a government's website, but it provides little information regarding the form and/or function of the government. The second methodological innovation we offer in our pipeline is designed to minimize the impact of this boilerplate text on the comparative analysis of government website content. 

Government websites provide information about how public policies shape the lives of local residents, and how local residents can engage with government to shape public policy. As such, government websites reflect both the results of, and inputs to, the political leadership in the city. In our illustrative application we explore the ways in which the contents of city government websites differ on the basis of the partisanship of the city's elected executive. A substantial body of research has found that the partisanship of the mayor affects city governance along multiple dimensions, including city budget priorities \citep{de2016mayoral}, policies affecting inequality in cities \citep{einstein2016mayors}, and framing of criminal justice policy \citep{marion2013mayor}. Furthermore, recent media coverage of changes to government websites that follow transitions in party control suggest that changes in web content are salient government actions, as perceived by the general public \citep{sharfstein2017science,kirby2017trump,duarte2017deniable} . We study whether significant differences between city governments based on mayoral partisanship are reflected in the contents of city websites.


\section{The Significance of Government Website Content}

 According to \cite{Mayhew1974}, politicians engage in advertising, credit claiming and position taking in order to get re-elected. Official city websites allow mayors to perform all three of these functions. Their offices frequently take a prominent position on the front page, and many websites also feature a picture of the mayor. We present an example of this in Figure \ref{fig:eriemayor}. The Erie, Pennsylvania website homepage presents an image of Democratic mayor, Joseph Schember, along with a list of laudable attributes of the city. In local politics, where campaign funds are low, this lends the incumbent a crucial advantage in becoming more well-known among her constituents. Furthermore, municipal politics gives incumbents clear and tangible achievements they can point to, such as completed infrastructure projects, the acquisition of federal or state funding, or the hosting of city-wide events. City websites present an opportunity for local officials to brandish these accomplishments. Finally, they also give mayors a platform from which they can advertise their political beliefs. On municipal websites, this may not manifest in the form of brazen partisanship, but more subtle avenues are available. As noted by \cite{einstein2016mayors}, there are stark differences in the spending preferences of Democratic and Republican mayors. City websites can then be used to communicate the stance of a mayor on social or economic programs. Another advantage of websites with regard to communication is that unlike direct social interactions, officials have full control over them.
 
\begin{figure}
\centering
\includegraphics[scale=0.35]{figures/eriemayor}
\caption{Screenshot from the homepage at \url{http://www.erie.pa.us/}, accessed on 06/14/2018. Image depicts  Democratic mayor of Erie, PA, Joseph Schember.}
\label{fig:eriemayor}
\end{figure}



Members of the public visit municipal government websites for a wide variety of purposes \citet{sandoval2012government}, and with significant regularity. In a survey conducted among a random sample of citizens in the state of Georgia in 2000---nearly two decades ago---found that 25\% of internet users reported visiting a local government website in the previous twelve months \citep{thomas2003new}. Furthermore, the use of a local government website is associated with an individual's perspective on government. \citet{tolbert2006effects} finds that users of local government Web sites are more likely to trust local governments, and hold other positive attitudes related to local and federal governments. Lastly, in a study of residents of Kansas City, Missouri, \citet{ho2017government} find that participants' perceived quality of the city website is strongly associated with their perceptions of the overall effectiveness of the City's communication with the public.


The literature making use of scraped websites clusters into a number of categories. One, and most pertinent to our own endeavors, the e-governance literature which discusses the online presence of governments from a usability and public service point of view. For the most part, research in this category develops a classification scheme to rate websites in terms of accessibility, ease-of-use and function, and then hand-codes a set of websites according to these criteria \citep{Urban2002,Armstrong2011,Feeney2017}. As an example, \cite{grimmelikhuijsen2012developing} study local government websites with the goal of uncovering how they aid the goal of transparency. To this end, they analyze a set of Dutch municipalities in which air quality had deteriorated. The authors test whether local governments provide citizens with information about potential complications and solutions associated with this issue. Like most e-government studies however, this publication does not make any use of automated text analysis.

Websites have also played a major role in the field of media studies, as scholars have scraped and analyzed the online presence of newspapers, as well as the more diffuse world of online political blogs \citep{Adamic2005,Gentzkow2010}. \cite{Lin2011} provide a good example for a study which makes extensive use of automated content analysis - a necessity arising from its dataset of 66830 blog posts and 57221 online news articles. The authors estimate the political slant of these entities by counting the frequencies with which politicians of either side are mentioned and determine that blogs are generally more biased. Unfortunately for us, the authors don't go into the details of their text analysis, and offer no information on the acquisition and pre-processing of the data.

Another well-known example fitting into this area of study is the set of studies conducted by King et al. \citep{KING2013,King2014,KING2017}, in which the authors study censorship by the country's government on its lively blogosphere. However, the authors also provide no information on how their data was collected ``our extensive engineering effort, which we do not detail here for obvious reasons [...]''.

The websites of politicians and their parties have also fallen under scholarly scrutiny. Researchers have found that in order to identify the constituencies, motives and modes of communication of these actors, their websites can be very illuminating sources of information \citep{Druckman2009,Druckman2010,Cryer2017,Esterling2011,Esterling2011a,Norris2003,Therriault2010}. \cite{Druckman2009,Druckman2010} rely on the \textit{National Journal} to find the websites, then hand-coded them. \cite{Cryer2017} provides fairly little information, but does mention the fact that she relied on Archive-it, a webservice of the Internet Archive. Unfortunately we found the data provided by the Internet Archive to not be sufficiently reliable and well-documented for our own purposes.
\cite{Esterling2011,Esterling2011a} rely on hand-coded data by the Congressional Management Foundation, a nonprofit organization which aims to assist Congress. \cite{Therriault2010} (a working paper) actually portends to use automated text analysis, and also has the most extensive overview of the associated methodology. However, the division of the website into sections (home page, topics, issues, details) is done by hand, and the actual analysis is incomplete. The author acquired the websites from the Library of Congress (which only collected them from legislators who actually consented, and Therriault notes that this causes nonrandom missingness).

Importantly for us, research analyzing and improving the scraping, pre-processing and analysis methods of this literature is scarce. \cite{Eschenfelder2002} provide something of an overview of how how federal websites should be assessed from an e-governance point of view, but they largely focus on the substantive criteria that should be fulfilled, rather than the technical aspects of website acquisition and analysis.

\section{Data}
In this section we introduce the data we use in our application---the analysis of municipal websites in six states - Indiana, Louisiana, New York, Washington, California and Texas. These states provide us with a sample that is well-balanced on a number of theoretically important indicators. One, each of the four geographic regions is represented with at least one state. Two, we have a fairly well-balanced sample with respect to the urban/rural cleavage, as both major cities less densely populated areas are covered. Furthermore, the sample is politically balanced -  we have three blue states (CA, WA, NY) and three red states (TX, IN, LA). Finally, our dataset contains some of the wealthiest states (NY, CA, WA and TX are \#2, \#8, \#9 and \#16 respectively, by GDP per capita \citep{BureauofEconomicAnalysis2017}), but also some of the poorer ones (IN and LA). In terms of pure GDP per capita, the sample is on the less affluent side - however, wealth is also correlated with poverty: CA is the state with the highest poverty rate in the country, and LA, NY and TX follow closely \citep{Fox2017}.

We acquired the website URLs from two sources: One, we scraped the URLs of city websites from their respective Wikipedia pages, which we found from lists of cities contained within each state. This method proved to be very reliable. Two, the General Services Administration (GSA) maintains all .gov addresses, and provides a complete\footnote{Domains used for testing and internal programs are excluded.} list of all such domains to the public through GitHub\footnote{https://github.com/GSA/data/tree/gh-pages/dotgov-domains}\footnote{This list is updated once per month - we rely on the version released on January 16, 2017. The data from the GSA contains the following data: One, domain name, specifically, the all-uppercase version of domain and top-level domain (for example, 'ABERDEENMD.GOV'). Two, the type of government entity to which the domain is registered, such as city, county, federal agency, etc. Three, for federal agencies, the name is specfied. Finally, the city in which the domain is registered, is noted.}. Naturally, this list does not contain cities which do not use a .gov website (or, in many cases, a city owns a registered .gov address, but uses a different one),. Furthermore, some of the links are non-functional, and some of the county websites on the list are incorrectly marked as city websites (and vice versa).

Since the GSA data is less complete and less reliable than the URLs found on Wikipedia, we mainly rely on the former, and only supplement them with the GSA data if a specific city doesn't have a URL recorded on Wikipedia, or our tests (see below) find it to be non-functional.

To test whether the websites we found actually work, we use a webdriver-controlled browser (Firefox/Selenium/Geckodriver). This is necessary because a) some city websites simply don't work, and more often, b) cities sometimes change their websites' URLs, in which case they redirect from the old to the new URL. A webdriver-controlled browser, unlike the more rigid conventional scraping tools, will simply follow this redirection. This allows us to subsequently record and use the new URL for the actual website scraping.

The partisanship of each city is coded in different ways, depending on the state. For Indiana, where elections are nominally partisan, this information is accessible through the state government's website\footnote{\url{http://www.in.gov/apps/sos/election/general/general2015?page=office&countyID=1&officeID=32&districtID=-1&candidate=}}. For Louisiana, we received data on the outcomes of mayoral elections from the LEAP project\footnote{\url{http://www.leap-elections.org/}}. For the other states, where mayoral elections are not nominally partisan (but the partisanship of the mayor is still well-known), we employed different means: For New York and Washington, we searched the state campaign finance websites, and recorded candidates who received money from party committees. For California and Texas, where our data consists of major cities, partisanship information was acquired from Ballotpedia\footnote{\url{https://ballotpedia.org/List_of_current_mayors_of_the_top_100_cities_in_the_United_States}}. Finally, we also scraped mayoral partisanship from the cities' Wikipedia pages. When compared to the other data sources above, (and manual searches in case of conflicts) this method once again proved to be very reliable, and added additional cases to our dataset even for Indiana and Louisiana. Generally speaking, we found data scraped from Wikipedia, aided by manual corrections in case of missing or conflicting data, to be more reliable than data from governmental sources.

One of the more subtle aspects of local government is the presence of different types of government structures. Between council-manager governments and mayor-council governments - either in the weak or strong mayor variant - there is a certain degree of variance in where a city's executive authority lies. Unfortunately we do not have access to information about the type of governments across the breadth of our dataset and therefore have to assume that the partisanship of a city's mayor corresponds to the partisanship of the entity responsible for determining the content on its website. Given the prominent place of mayors tend to have on their cities' websites, as well as the fact that council-manager city governments tend to appoint mayors rather than elect them (meaning there are unlikely to be many in our dataset), we feel that any bias arising from this nuance should be minor at the most.

Information on other covariates (population and median household income - from the American Community Survey 5) was acquired through the API of the U.S. Census Bureau\footnote{\url{https://www.census.gov/data/developers/data-sets.html}}.
\input{tables/tabCitiesStatesParties.tex}


\input{tables/tabFiletypeFrequencies.tex}

For some cities, whose websites make heavy use of JavaScript, this method does not lead to satisfying results. Consequently we restricted our corpus to cities with at least 3 documents.


\section{The Web to Text Pipeline}
In this methodological pipeline from native website files to text data that is appropriate for comparative analysis, we address two methodological challenges. First, though they contain significant amounts of text, websites are not comprised of clean plain text files. Rather, the files available at websites are of multiple types, including HTML, PDF, word processor, plain text, and image files. The first step in the methodological pipeline is aimed simply at extracting clean plain text from this heterogeneous file base. The second step in our methodological pipeline is to process the text to remove boilerplate language---language that is effective at differentiating one website from another, but is uninformative regarding policy or process differences between governments. We describe these methodological steps in this section.

\subsection{Site to Text Conversion}
For the most part, the file type of a document can be correctly determined through its ending. However, there are exceptions to this, which, if ignored, can lead to large amounts of garbage text, arising from incorrectly converted documents, which leads to a general decrease in the amount of usable data. Two issues in particular need to be addressed: One, HTML files on city websites frequently do not have an ending, but are still perfectly readable if correctly identified as such. Second, some documents contain the incorrect file ending - for example, we found thousands of documents that ended in .html, when they were actually PDFs. To accurately assess their type, we rely on the R package wand, which is an R interface to the Unix library libmagic, which determines the type of a file on the basis of its file signature. Consequently we rename all documents so that their file ending reflects their actual file type. This is strictly necessary, because we rely on the readText R package\footnote{We have also experimented with several Unix-based alternatives, but found that they largely led to the same results.} - which determines a document's type solely through its ending - to convert the files to plain text.

The text documents are then read into R line by line, converted to UTF-8 and then stripped of dates, punctuation, numbers and words connected by underscores. At this point, the documents of one city still closely resemble one another in the form of boilerplate content, be it website elements (i.e. "You are here", "Home", "Directory" etc.) in html documents, or commonly used forms or phrases in pdfs, doc and docx files. This is an issue, because it clusters documents around the cities from which they originate in a way that has nothing to do with their actual content. In other words, the signal would be drowned out by the noise. Our solution to this problem is described in more detail in section \ref{boilerplate}.
Preprocessing further includes setting every character to lowercase, as well as the removal of bullet points which frequently occur in html documents, extraneous whitespace, xml documents mislabeled as html files, and empty documents. Furthermore, some documents contain gibberish, often as a result of faulty or impartial OCR. To combat this problem, we employ two solutions. One, we use spellchecking, implemented through the hunspell R package, to remove all non-English words.\footnote{Some of the cities, for example Los Angeles, do contain a sizable proportion of Spanish content. The analysis of this content is beyond the scope of this paper, but could be explored in future work, for example relying on multilingual word embeddings. Since the removal of non-English words is very computationally-intensive, we only take this step at the end of the preprocessing process, the result of which might be a slightly adverse effect on the accuracy of the boilerplate classifier.} However, hunspell does not cover everything, either because some tokens are not actual words (for example artifacts from defective encoding), or because random sequences of characters just so happen to form words that exist in a dictionary (for example "eh" or "duh"). Since we rely on a bag-of-words model in which syntax does not matter, we can ameliorate these problems by removing all text except for whitespaces and the characters that appear in the English alphabet. Since a lot of the nonsensical text tends to be quite repetitive, we also delete all documents in which the proportion of unique to total number of tokens is less than 0.15. Furthermore, hunspell does not spellcheck individual characters or two-character words, so we remove these token types entirely (none of these words are of any substantive relevance to our research question). Since these pre-processing steps reduce documents which are largely unsuitable to only a few words of texts that don't make much sense, we also remove all remaining documents containing less than 50 tokens. Finally, to remove words that are extremely rare (which also has the advantage of eliminating any remaining oddities) and thus add nothing substantive to our models while increasing their computational cost, we also discard any token types that occur in only one document. We also conduct lemmatization to reduce words to their basic form.

\subsection{Boilerplate Removal}\label{boilerplate}
As noted above, city websites contain a large amount of text that is uninformative for its actual content and therefore a hindrance to correct analysis by automatic text processing methods. This is a common issue with textual data in which informative content is embedded in technically structured documents. See, e.g., \citet{burgess2016legislative,wilkerson2015tracing} and \citet{linder2018text} for examples of boilerplate removal in the analysis of legislative text. In the case of websites, lines in documents are generally quite informative, so all of our boilerplate removal efforts are done at this level.

%this is outdated; not deleting it yet in case we need it for further reference
%Consequently we remove this content as following: Each line of every document is compared to every line in every other document belonging to the same city. We count how many times each line is duplicated for that city. We remove any line occuring more than our chosen threshold of 10.\footnote{Empirically, lines tend to be duplicated either hundreds of times, or only once or twice, if at all.} This means that each document only retains the information that is particular about it. We implement this algorithm through hash tables, which reduces the computational complexity from O($N^2$) to O($N$). Before this step is taken, we remove numbers and dates from the documents because they frequently make lines unique, despite the fact that they are virtually the same (for example different days on a city calendar).

\subsection*{Boilerplate Classification}
In order to determine whether a line should be discarded, we train a simple classifier. We sampled 100 lines from documents each of the following five cities: Los Angeles, CA, Indianapolis, IN, New York, NY, Shreveport, LA, and Seattle, WA. To ensure that lines which occur more frequently in these cities (sometimes hundreds of thousands of times) had a higher probability of being scrutinized by the classifier, we use sampling weights equivalent to the proportion of total lines in a city's corpus made up by each specific line type. To account for the higher likelihood of some lines being part of the training set, we use inverse probability weights in the classifier.\footnote{Note that the performance of the classifier is robust to the use of these weights and only changes by about one percentage point if they are not used.}

These 500 lines were then hand-coded as either substantively useful or useless. Then we trained a random forest with this usefulness measure as the dependent variable. The independent variables were: (1) number of times the line was duplicated within the city, (2) length of the line, in characters, (3) number of tokens in the line, and (4) the median distance from the document midpoint to the position of the line itself. The purpose of these covariates is as following:

The length of the line and the number of tokens are a way to find lines consisting of only a word or two. This is highly predictive of lines which are used as website headers and navigational elements, which are of of zero substantive interest to us. These terms also happen to be fairly common, which causes them to be overweighted by the topic model.

To directly address the latter problem, a measure for the number of times a line is duplicated within a city is included. Many lines occur hundreds or even thousands of times on a single website, and therefore are terms that are highly predictive of the website, which causes the topic model to create topics that are highly correlated with cities.

Finally, the distance measure: Since boilerplate terms such as navigational elements, headers, footers, and so on, should occur more frequently at the beginning and the end of websites, we attempt to identify such content as following: We measure the distance between the midpoint of a document and the position of a line, expressed as quantiles (to account for differing document lengths). Since lines can occur in multiple documents, or multiple times in the same document, we take the median of these measures. Thus, for example, a line which often occurs at the beginning of documents might have a score of 0.45, whereas a line that tends to be found more in the center, and thus be indicative of more relevant content, might be scored with a 0.11 instead.

We rely on random forests as a classifier, which offer slightly better performance than logit\footnote{We also tried SVM, boosted trees and AdaBoost, with similar results and chose the random forests because this method has a probabilistic basis and is more intuitive.} and have the added benefit of giving estimates of variable importance. Performance of this classifier was assessed through five-fold cross-validation, the results of which can be found in table \ref{randomForest}.

%Finally, the keyness measure: This indicator shows whether a term occurs disproportionally frequently in one document collection, compared to another. In our case, this would be one city's documents, compared to all others. Mathematically, this is a simple chi-square test, asserting whether the frequency of a word in a city is greater than its expected count, as determined by its count in all other documents, and the number of tokens in the city. The variable used in the model is the chi-square value of this test for each word (which varies and is therefore calculated individually for each city).\footnote{Or alternatively, the log of the absolute chi-square value, after which the previously negative values are made negative again.}

%A regression table for this model can be found in table 3. The associated accuracy in 5-fold cross-validation is 0.8325. The cross-validation accuracy of a simplified model with only the number of characters as a covariate is 0.825 (table 4). A model with the four covariates described above as well as interaction terms between each of them yields an accuracy of 0.885 (table 5).

%\input{tables/boilerplateClassifier1.tex}
\input{tables/boilerplateClassifierRFMetrics.tex}
%\input{tables/boilerplateClassifierRFImportance.tex}

This classifier is then used to flag and remove all lines that are not classified as substantively useful. The effect of this process on the corpus is illustrated with the corpus of Anchorage, AK (i.e. a city that isn't part of our sample used in the analysis) as an example in Figures \ref{boilerplate_before_after1} to \ref{boilerplate_before_after4}. Before the lines identified by the classifier as boilerplate are removed, lines with very few characters and words are the most common. After the removal, the distribution looks more like what it should be - lines of medium length now occur more frequently than extremely short ones (see figures \ref{boilerplate_before_after1} and \ref{boilerplate_before_after2}). Furthermore, lines that are duplicated only a few times rather than dozens, hundreds or even thousands are now more common (see figure \ref{boilerplate_before_after3}). Finally, the position of the line within the documents is not as important to the random forest, and this also shows in the results. However, this feature still has a positive effect, as lines at either end of the document are a bit less common now (see figure \ref{boilerplate_before_after4}). After all the preprocessing is set and done, our corpus consists of 259,099 documents.

\begin{figure}[htp]
	\centering
	\caption{Effects of the boilerplate classifier on the corpus of the city of Anchorage, AK.}
	\label{boilerplate_before_after1}
	\includegraphics[width=\linewidth]{figures/boilerplateBeforeAfterNwords.pdf}
\end{figure}
\begin{figure}[htp]
	\centering
	\caption{Effects of the boilerplate classifier on the corpus of the city of Anchorage, AK.}
	\label{boilerplate_before_after2}
	\includegraphics[width=\linewidth]{figures/boilerplateBeforeAfterFreq.pdf}
\end{figure}
\begin{figure}[htp]
	\centering
	\caption{Effects of the boilerplate classifier on the corpus of the city of Anchorage, AK.}
	\label{boilerplate_before_after3}
	\includegraphics[width=\linewidth]{figures/boilerplateBeforeAfterMedianDocMidDist.pdf}
\end{figure}
\begin{figure}[htp]
	\centering
	\caption{Effects of the boilerplate classifier on the corpus of the city of Anchorage, AK.}
	\label{boilerplate_before_after4}
	\includegraphics[width=\linewidth]{figures/boilerplateBeforeAfterNchars.pdf}
\end{figure}

\section{Bag-of-Words Text Analysis}

We illustrate the analysis of municipal website content using bag-of-words (BoW) methods. BoW methods are methods of text analysis that do not take into account the sequence or placement of words in text---just the presence and frequency of words. As noted by \cite{GrimmerStewart2013}, for most applications, bag-of-words approaches have been found to be more than sufficient. Furthermore, there is reason to believe that city government websites are a particularly `safe' case for bag-of-words methods due to their informative, manner-of-fact based language. It is extremely unlikely for these pages to feature ambiguous language such as an abundance of negation or even sarcasm.

\subsubsection{Structural topic model}
A powerful and frequently used approach within the family of BoW methods is the use of topic models. This class of clustering methods relies on the co-occurrence of words within documents to form a set of semantically coherent topics. In order to compare the degree to which Republicans and Democrats prefer specific topics, we rely on the structural topic model, developed by \citep{Roberts2014}. Theoretically, the most widely-used form of topic model, latent dirichlet allocation, can also be used to test for the impact of a single covariate through a post-hoc comparison, but the structural topic model allows for multiple covariates, and also produced more meaningful topics in our experiments.

We use 60 topics - the number recommended by the authors for medium- to large-sized corpora. Since our corpus is at the larger end of that spectrum, the appendix also contains the results of a model with 120 topics, which corroborates the findings of the one presented here. We use four covariates: First, \textit{party}, to test our main hypothesis. Second, \textit{city population}, which the literature frequently emphasizes as a determinant of the issues a city faces (see, for example, \cite{Guillamon2013}). Third, we control for wealth by relying on \textit{median income} as a covariate. Fourth and finally, we include a \textit{state} variable.

The results are shown in table \ref{stmResultsTable}. The rows of the table are sorted so that the most Republican topics (marked by a deeper red color) appear at the top, and the most Democratic ones (blue) at the bottom. The topics that are entirely white are not statistically significant for the party covariate. In order to test statistical significance, we calculated credible intervals - the threshold chosen here is the 1\% level.

Many of the topics associated with Democrats - one related to education, one to recycling  - clearly seem to match the party brand. For example, topic 44 on affordable housing clearly resonates with the Democratic party's appeal to low-income voters. Similarly, employee rights are represented in topic 46. Democrats also exhibit a strong preference for words related to public finances, such as topic 22 ('budget', 'revenue', 'expenditure') as well as topic 17 ('debt', 'bond', 'financial'). We hypothesize that this is indicative of a greater willingness to emphasize the city's efforts to raise and spend money. This finding is consistent with \citep{Einstein2015}, who show that Democratic mayors tend to favor greater spending. Democrats are the party of big government, and this clearly also shows at the city level. A second, conistent Democratic focus appears to be law enforcement: The most Democratic topic, 57 ('robbery', 'homicide', 'sergeant') (a comparable topic is also the most Democratic topic in the model with 120 topics in tables \ref{stmResultsTable120_1} and \ref{stmResultsTable120_2} of the Appendix) depicts Democrats' complicated relationship with law enforcement - a finding that is not entirely without precedent in the literature (see \citep{Einstein2015}). Finally, Democrats also focus more on the deliberative process of good governance, as topics 29 ('election', 'agenda', 'committee') and 37 ('audit', 'procedure', 'oversight') attest to.

Republicans, meanwhile, live up to their reputation as the party of small government. Many topics prevalent in Republican cities focus on the bare essentials. Basic utilities such as energy (topic 7), fire protection (topic 33), drinking water (51) and garbage removal (topic 49) are ubiquitous here. Similarly, protecting citizens from natural disasters is a focus in topics 1 ('storm', 'runoff', 'drainage') and 20 ('snow', 'hurricane', 'tornado') is an important issue.



%Interestingly enough, Democrats also `own' the topic related to law enforcement, which might be somewhat unexpected given Republicans' usual focus on law and order \citep{Gerber2011a}. However, this kind of finding is not entirely without precedent in the literature (see \citep{Einstein2015}). Similar to the informed dirichlet model, the structural topic model also finds the emphasis on construction and infrastructure by Republicans - in table \ref{tabSTMINRep}, topics 2, 7 and 8 clearly focus on these issues.\footnote{The first Repuiblican topic in Indiana (library, stream, obj, etc.) is likely an artifact from incorrectly converted html, and since it presumably only happens only in one Republican city, the topic is classified as very Republican.}

%When comparing Indiana to Louisiana, it appears that the Democratic emphasis on law enforcement is robust. Furthermore, as with the fightin' words approach, some smaller degree of focus on money (see topic 1) is still evident. For Republicans, topics 2 to 4 seem to be, once again about infrastructure and utilities, pointing to a certain degree of robustness in these results, as well as the emergence of a trend. The results produced by the structural topic model are not flawless, but the two parties do seem to have somewhat consistent themes on which they focus on in both states. Furthermore, in comparison to the fightin' words approach, the ability of the structural topic model to form coherent topics is quite evident and helpful in the interpretation of the results.

%An ostensibly intuitive solution to topics clustering into cities in LDA is to include dummies for the cities in a statistical model of topics. This is facilitated by the structural topic model, which uses metadata on the document to account for variation in topics \citep{Roberts2014}. However, figure \ref{stm_results} shows that if anything, the STM exacerbates the problem. Here, we plot the \textit{p-values} of the coefficients for each city as well as the party variable across each topic. Under normal circumstances, plotting the p-values, as opposed to the fitted values, does not make much sense, but here it serves a diagnostic purpose. The plot shows that the party variable is never statistically significant at any conceivable level of confidence, nor is it even close to. Interestingly the same is true for a number of the cities as well. The topics cluster heavily into only about half of the cities, which does not present an improvement over LDA at all.

%Partisan top words - stm Louisiana -- Rep
%\input{tables/stmTopWordsINRep.tex} %\ref{tabSTMINRep}

%Partisan top words - stm Louisiana -- Dem
%\input{tables/stmTopWordsINDem.tex} %\ref{tabSTMLADem}

%Partisan top words - stm Louisiana -- Rep
%\input{tables/stmTopWordsLARep.tex} %\ref{tabSTMLARep}

%Partisan top words - stm Louisiana -- Dem
%\input{tables/stmTopWordsLADem.tex} %\ref{tabSTMLADem}

%\subsubsection{Prediction with SVM}
%An alternative approach to the problem is to ignore topics entirely, and go straight to predicting documents that are much more likely to be included on websites belonging to one or the other party. Classic machine learning techniques such as Naive Bayes, Linear Discriminant Analysis, or SVM should be expected to fare well in this context. Here, we rely on SVM, implemented with the SciKitLearn package in Python.\footnote{We also implemented SVM in R through the packages kernlab and e1071 in R. However, neither of these provide a regularized version of SVM (NOTE: at least that is what I am gathering from the stack overflow error), which prevents us from using all of the features contained in our data. Instead, we ranked the features according to tf-idf and selected the top 5000. These methods are also quite slow, and provide a maximum accuracy of 82\% in five-fold cross validation.} A grid search reveals the tf-idf representation of the document-term matrix to be better than pure word counts, unigrams to be superior to bi-grams, the application of an L2 penalty to be preferable to either L1 or elasticnet, and an alpha (a constant to multiply with the regularization parameter C) of 0.0005 to lead to the best results. Applying five-fold cross-validation to the (tf-idf) document-term matrix with the dimensions 16011x35000 leads to an average accuracy of 89\%.\footnote{Other methods used: Elastic-net in the glmnet package in R. Accuracy is 0.6924795 for in-sample prediction, so not worth bothering with.}

%However, \cite{Monroe2008} advise against using these types of methods in this context because they get the data generation process backwards: Our theory assumes that party leads to variation in writing, and yet we rely on the documents to predict party, in spite of the fact that we actually have perfect knowledge of it.



%Heatmaps
%\begin{figure}[htp]
%	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%	\caption{Word-topic probabilities for topics with big partisan differences, across documents (Indiana).}
%	\label{heatmaps_weights}
%	\includegraphics[width=0.8\linewidth]{figures/heatmaps_weights_IN.png}
%\end{figure}



%US map
\begin{figure}[htp]
	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
	\caption{Cities in the corpus, by partisanship of mayor. \textbf{REVISE to remove everything not in our six states.}}
	\label{us_map}
	\includegraphics[width=\linewidth]{figures/us_map.pdf}
\end{figure}

%stm results
%\begin{figure}[htp]
%	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%	\caption{Results from a structural topic model, displayed as the p-values for each variable for each topic. This would normally be somewhat nonsensical, but here it illustrates why the model does not work.}
%	\label{stm_results}
%%	\includegraphics[width=1.1\linewidth]{figures/stm_results.pdf}
%\end{figure}



%Partisan top words - topic model Indiana
%\input{tables/partisanTopWords.tex} %\ref{tabFightinIN}

%Partisan top words - topic model Louisiana
%\input{tables/partisanTopWordsLA.tex} %tabLDALA

%Partisan top words - stm Indiana
%\input{tables/stmTopWordsIN.tex} %\ref{tabSTMLA}

%Partisan top words - stm Louisiana
%\input{tables/stmTopWordsLA.tex} %\ref{tabSTMLA}


%Some basic descriptive statistics of documents by party
%\input{tables/descriptiveStatisticsPartisanIN.tex}

%Some basic descriptive statistics of documents by party
%\input{tables/descriptiveStatisticsPartisanLA.tex}

%fightin words results
%\begin{figure}[htp]
%	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%	\includegraphics[width=\linewidth]{figures/linesCutoffIN.pdf}
%		\caption{Total number of lines retained at a given threshold for removing duplicated lines. For example, at x = 10, all lines occurring more than 10 times within a city's documents are removed.}
%	\label{linesCutoff}
%\end{figure}


%\section{Ground truth test}
%In the realm of public administration, the notion that the partisan leaning of mayors might have an effect on how they run their cities is still frowned upon to some extent. Perceived more as managers than politicians, they have been portrayed as the last bastion of non-partisanship in America, and in many cases, also style themselves that way \citep{Dovere2018}. However, the aspirations some mayors have shown towards higher offices - in some cases even the presidency - reveal that they are not quite as above the fray as some may believe them to be. One of the most vicious and blatantly partisan cleavages in current U.S. politics - the debate surrounding sanctuary cities - has seen mayors in a central role. Research into local politics has shown that partisan elections consistently have greater turnout \citep{Schaffner2001}. When voters are denied this cue, they make use of other, and considerably more irrational heuristics, such as name, gender, or occupation of the contenders. Consequently it only makes sense for any office-seeking politician to emphasize their partisanship. Finally, decades of research in political psychology have consistently shown that no matter how hard we try, humans are simply incapable of escaping our partisan biases, a finding that is especially pronounced among elites \citep{Hatemi2011}.
%
%In an effort to underline this fact and remove any doubt about the fact that the partisanship of mayors colors their decision-making, we conduct a ground truth test between our main corpus - the websites of cities - and a second, decidedly more partisan set of texts: the campaign websites of these mayors. As noted above, partisanship has been shown to be a powerful driving force even in local politics, and mayors are incentivized to exploit it. Consequently they are very likely to emphasize conservative/liberal values on this platform. If there is a greater correlation in word use between the cities managed by a party and the campaign websites of its mayors than with those of the other party, evidence for the partisanship of city websites can be established.
%
%Using the same methods as described for our main corpus, we have gathered these sites and then concatenated all of the documents belonging to mayors of the two parties into one ground truth document each. We do the same for the city documents, and the compare the four document collections using cosine similarity. This measure is the cosine between the angle of two vectors, in this case the frequencies of all words in the two vocabularies. Compared to a simple euclidean distance, this has the advantage of accounting for the fact that the two corpora being compared are not necessarily of the same length. The cosine measure between two documents ranges between 0 and 1, 0 signifying absolutely no correlation, and 1 perfect overlap. Figure \ref{groundtruth} shows the result of this test. The expectation is for a greater similarity between Republican cities and the Republican ground truth, than Republican cities and Democratic ground truth - and vice versa. At present however, this does not appear to be the case, presumably because the Republican ground truth consists of 8 documents, and the Democratic one of 290.

%ground truth test
%\begin{figure}[htp]
%	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%	\includegraphics[width=\linewidth]{figures/groundtruth_corrplot.png}
%	\caption{Ground truth test. The values are cosine similarities between a pair of document collections.}
%	\label{groundtruth}
%\end{figure}
%
%%ground truth test
%\begin{figure}[htp]
%	\centering % Using \begin{figure*} makes the figure take up the entire width of the page
%	\includegraphics[width=\linewidth]{figures/groundtruth_bs_bigcities_corrplot.png}
%	\caption{Ground truth test. The values are cosine similarities between a pair of document collections (top 100 mayors vs. IN and LA).}
%	\label{groundtruth}
%\end{figure}

%Ground truth test between top100 mayors and IN + LA; with bootstrapped confidence bounds
% label: groundtruth_bootstrapped
%\input{tables/groundtruth_bootstrapped.tex}


%\input{tables/stmTopWords.tex} %\ref{tabSTMtopwords}


\input{tables/stmTopWords2.tex} %\ref{tabSTMtopwords2}
\input{tables/stmTopWords120.tex}
\input{tables/stmTopWords120_2.tex}

\section{Conclusion}

We have developed a methodological pipeline for automatically gathering and preparing government websites for comparative analysis. This methodology holds the potential to vastly scale up the data collection efforts underpinning the rapidly growing body of research that is focused on government website analysis. Through an application to the analysis of municipal websites in six different states, we show how our pipeline is capable of gathering corpora that shed light on the forms and functions of local government.


%\input{tables/tabCitiesStatesParties.tex}
%\input{tables/stateUrlSummary.tex}

\newpage

\bibliographystyle{apsr} % apsr stopped working for me
%\bibliographystyle{plainnat}
\bibliography{ref}

\newpage
\section*{Appendix}


\begin{figure}[htp]
	\centering
	\caption{Five largest topic effects for the population covariate. The fact that the population and epidemiology topics are positively correlated with city size is indicative of the model's validity.}
	\label{stmEffectPop}
	\includegraphics[width=\linewidth]{figures/stm_effect_pop.pdf}
\end{figure}

\begin{figure}[htp]
	\centering
	\caption{Five largest topic effects for the median income covariate. The fact that the crime topic is most prevalent in poorer cities, good governance is the most positively correlated with income is indicative of the model's validity.}
	\label{stmEffectIncome}
	\includegraphics[width=\linewidth]{figures/stm_effect_income.pdf}
\end{figure}

\end{document}





%\begin{enumerate}
%	\item Design
%	\begin{enumerate}
%		\item Choosing the sample
%		\item Finding URLs
%		\item list of .gov websites
%		\item Finding supporting data
%	\end{enumerate}
%	\item Scraping
%		\begin{enumerate}
%			\item wget
%			\item headless browser/Selenium
%			\item Beautifulsoup/rvest
%			\item APIs (httr)
%			\item Wayback Machine
%		\end{enumerate}
%	\item Pre-processing
%		\begin{enumerate}
%			\item Determining document filetype
%			\item File conversion
%			\item Conventional preprocessing (lowercase, numbers, punctuation)
%			\item Stemming and lemmatization
%			\item spellchecking
%			\item Dealing with duplicate text \& html documents in particular
%		\end{enumerate}
%	\item Analysis
%		\begin{enumerate}
%			\item LDA
%			\item Other topic models (structural, author, dynamic -- maybe?)
%			\item SVM (+ other machine learning classifiers?)
%			\item Fightin Words
%		\end{enumerate}
%\end{enumerate}
%
%
%
%
%
%
%% latex table generated in R 3.3.3 by xtable 1.8-2 package
%% Wed Mar 22 11:32:43 2017
%%\begin{table}[ht]
%%	\centering
%%	\begin{tabular}{lrrlrrl}
%%		\hline
%%		City & DemVotes & RepVotes & Winner & Change & Pop15 & url \\ 
%%		\hline
%%		Attica &  & 187 & Republican & 0 & 3117 & https://attica-in.gov/ \\ 
%%		Connersville & 1005 & 995 & Democratic & 1 & 13010 & http://connersvillecommunity.com/ \\ 
%%		Frankfort &  & 1748 & Republican & 0 & 16060 & http://frankfort-in.gov/ \\ 
%%		Huntingburg & 447 & 793 & Republican & 0 & 6035 & http://www.huntingburg-in.gov/ \\ 
%%		Indianapolis & 92830 & 56661 & Democratic & 1 & 862781 & http://www.indy.gov \\ 
%%		Lake Station & 1483 & 227 & Democratic & 0 & 12054 & http://www.lakestation-in.gov/ \\ 
%%		Linton & 785 & 692 & Democratic & 0 & 5284 & http://www.linton-in.gov/ \\ 
%%		Madison & 1192 & 1915 & Republican & 0 & 12040 & http://www.madison-in.gov/ \\ 
%%		Mitchell & 229 & 495 & Republican & 1 & 4252 & http://mitchell-in.com/ \\ 
%%		Monticello & 0 &  & Democratic & 0 & 5322 & http://www.monticelloin.gov/ \\ 
%%		North Vernon & 679 & 697 & Republican & 1 & 6619 & http://www.northvernon-in.gov/ \\ 
%%		Richmond & 3421 & 2731 & Democratic & 0 & 35854 & http://www.richmondindiana.gov/ \\ 
%%		Rockport & 286 & 272 & Democratic & 1 & 2223 & http://www.cityofrockport-in.gov/ \\ 
%%		South Bend & 8515 & 2074 & Democratic & 0 & 101516 & https://www.southbendin.gov/ \\ 
%%		Union City & 338 & 440 & Republican & 0 & 3447 & http://www.unioncity-in.gov/ \\ 
%%		Winchester & 606 & 524 & Democratic & 1 & 4769 & http://www.winchester-in.gov/ \\ 
%%		\hline
%%	\end{tabular}
%%	\caption{} 
%%\end{table}
%
%\subsection{Research Design}
%
%\begin{table}[ht]
%	\centering
%	\begin{tabular}{llr}
%		\hline
%		Variable & Unit & Source \\
%		\hline
%		Population size & 1000 people & Census \\
%		Population growth last 5 years & Percent & Census \\
%		Type of economy (agriculture/industry/services) & ? & Census \\
%		Economic performance (GDP?) & \$ & Census \\
%		Party of mayor before election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
%		Party of mayor after election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
%		Change of party control & 0/1 & in.gov/sos/elections/ \\
%		Presidential vote 2012 in county & Percent Rep & ? (but I have the data) \\
%		Unemployment rate & Percent & Census \\
%		Broadband speed & Avg. Mbps DL & broadbandmap.gov \\
%		\hline
%	\end{tabular}
%	\caption{List of covariates} 
%\end{table}
%
%
%
%\begin{enumerate}
%\item Corpus:
%\begin{enumerate}
%\item Last snapshots before the election (November 3, 2015 in Indiana; tbd. in Louisiana (probably February))
%\item First snapshot that is at least 2 months after the new government's inauguration (which is in January for Indiana, May for Louisiana)
%\end{enumerate}
%\item Preprocessing:
%\begin{enumerate}
%\item restrict corpus to:
%\begin{enumerate}
%\item documents belonging to cities in which a change of power occurred
%\item documents that were added, deleted or changed between the two snapshots
%\end{enumerate}
%\item words to lowercase
%\item remove punctuation
%\item stemming (Porter stemming algorithm?)
%\item Remove stop words (regular list of stop words is enough, since we use an asymmetric prior)
%\end{enumerate}
%\item Apply Grimmer's expressed agenda model to the corpus
%\begin{enumerate}
%\item Asymmetric prior
%\item Each document can have only one topic (in contrast to the author-topic model)
%\item Cities $i = 1,..., n = 15$
%\item Topic $k(k = 1,..., K )$
%\item Documents $j(j = 1,...,D_i)$ from city i
%\item Party covariate in the prior, where the deleted and unmodified documents are coded as from the first, and the added and modified documents from the second party
%\end{enumerate}
%\item Results
%\begin{enumerate}
%\item Label topics using Grimmer's automatic cluster labeling method, based on most commonly used words in documents belonging to topic
%\item Evaluate topics
%\end{enumerate}
%\end{enumerate}
%
%Validation:
%
%\begin{itemize}
%\item Do the above for cities in which no change of power occurred.
%\item Check whether there is higher than average turnover around the new year by comparing changes to non-election years (and also Louisiana, where elections are later).
%\item Check how long documents stay on websites on average. Use websites with a lot of snapshots for this (these exist for both small and large cities).
%\end{itemize}
%
%Problem with using this model: Grimmer's expressed agenda model uses Senators as the actors. Senators is also who he is substantively interested in. For us, the equivalent to Senators is cities. However, we care about parties, not cities.
%
%\subsection{Survival model}
%The existence of individual documents on municipal government websites can be though of as a survival process. No document stays on a website forever, and it appears to be a reasonable assumption that as documents get older and thus less relevant, they get replaced. The factors determining the steepness of the survival curve are the topic - fire safety regulations likely stay up longer than a bulletin on the annual spring banquet - and the change of party control after an election.
%
%\begin{quote}
%\textit{H1}: The older a document, the more likely it is to be removed.
%\end{quote}
%
%$S(t)$ has a downward slope. Admittedly, this is almost impossible not to be true. Also, test proportional, rising and falling hazard models.
%
%\begin{quote}
%\textit{H2}: Documents pertaining to administrative matters are less likely to be removed.
%\end{quote}
%
%Introduce a categorical variable for the top 10(?) topics. A negative coefficient for administrative topics would support this hypothesis.
%
%\begin{quote}
%\textit{H3}: Documents introduced by the opposing party are more likely to be removed.
%\end{quote}
%
%Introduce two variables into the survival model: One variable indicating which party has introduced a document, and a time-varying variable describing which party is currently in government. The hypothesis is tested through an interaction term between the two.
%
%\begin{quote}
%\textit{H4a}: Democrats are more likely to remove documents with topics pertaining to private enterprise, private schools.
%\end{quote}
%
%Interaction term between party in power and categorical topic variable.
%
%\begin{quote}
%\textit{H4b}: Republicans are more likely to remove documents with topics pertaining to social justice, equality, taxes, public schools, etc.
%\end{quote}
%
%Interaction term between party in power and categorical topic variable.
%
%\begin{quote}
%\textit{H5}: In line with their commitment to small government, Republicans are more likely than Democrats to remove documents.
%\end{quote}
%
%Party in power variable.\\
%
%
%This model will take up a lot of degrees of freedom. The rarity of snapshots for some cities might be a problem. Documents being changed and being removed can be modeled as competing risks.
%
%
%\begin{equation} 
%\label{eq1}
%\begin{split}
%Y & = \text{Party that introduced the document} \\
% & + \text{Party that is currently in power} \\
% & + \text{Topic 1, topic 2, ..., topic k} \\
% & + \text{Party that is currently in power} \times \text{Topic 1, topic 2, ..., topic k} \\
% & + \text{Days since start of mayoral term (control)}
%\end{split}
%\end{equation}
%
%
%
%
%
%After the counties, townships and cities that cannot be matched to the Census data\footnote{There are five cities that are not contained in the Census data} and duplicate websites (some cities have more than one website) are removed, 1813 domains/cities remain.
%
%These cities contain 90,616,865 people, and thus about 28\% of the U.S. population (see figure 1).
%
%\begin{figure}[htp]
%	\centering
%	\caption{Percentage of state population covered.}
%	\includegraphics[width=0.9\linewidth,height=0.9\textheight]{figures/coverage_states.pdf}
%\end{figure}
%
%We use the resulting list of websites to acccess their copies stored in the Internet Archive's Wayback Machine. To this end, we rely on the Ruby Gem 'Wayback Machine Downloader'\footnote{https://github.com/hartator/wayback-machine-downloader} (WbMD). We supply the URL that each .gov website redirects to to the WbMD, which then downloads every file present in the WbM from a snapshot in October 2016, or, if not available, as soon as possible after this point.
%
%<Note: We have not actually done this last step for all websites (however, the R script which runs the Ruby package is already set up to do so once we need to). Instead 10 websites were randomly sampled from an older version of the GSA list, which still contained counties and townships, which is why one of the 10 websites is from Dutchess County, NY.>
%
%It would be fine to focus on Indiana as a case. First, we need to answer some preliminary questions about the data.
%
%\begin{enumerate}
%
%\item For what percentage and number of IN cities can we find data from the WBM?
%\item For how many election cycles can we find political leadership data for these matched cities?
%\item In what number and percentage of cities is the local leadership majority Republican? 
%\item Relatedly, in a typical election cycle, for how many cities do we see a transition in party leadership (i.e., a shift from majority D (R) to majority (R) D). 
%
%\end{enumerate}
%
%\begin{enumerate}
%	
%	\item 30 cities, with a combined population of 1,180,435. However, since only cities (as opposed to towns and villages) hold mayoral elections, only 16 of these, with a combined population of 1,094,383 can be matched to the election data.
%	\item 2015, 2011, 2007, 2003.
%	\item Of the 16 cities, 7 have Republican mayors after the 2015 elections.
%	\item In 6 cases, a shift of party control occurs, with 4 of these being Republican --> Democratic. 
%	
%\end{enumerate}
%
%
%
%
%\section{Running Application: Party Differences in Municipal Websites}
%
%
%\begin{landscape}
%\begin{table}[htbp]
%	%\caption{}
%	\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%		\hline
%		Names & Year & Journal & Findings & Important? \\ \hline
%		Benedictis-Kessner, Justin De
%		Warshaw, Christopher & 2016 & JOP* & Regression discontinuity design. Democratic mayors spend more (but it is unclear on what, not the typical Democratic issue-areas), issue more debt, pay more interest & Yes \\ \hline
%		Caughey, Devin
%		Warshaw, Christopher
%		Xu, Yiqing & 2015 & Working Paper & Regression discontinuity design. Partisan composition of state governments affects state policy liberalism (composite index for the areas of social welfare, taxation, labor, civil rights, womenâs rights, moral legislation, family planning, environment). & Somewhat \\ \hline
%		Einstein, Katherine Levine
%		Kogan, Vladimir & 2015 & Urban Affairs Review & Cities with more Democratic citizens spend more; more progressive (rather than regressive) forms of taxation; pursue intergov. aid more; spend more on police, fire, parks \& recreation & Somewhat \\ \hline
%		Einstein, Katherine Levine
%		Glick, David M. & 2015 & Working Paper & Survey of 72 mayors. Unlike Republican mayors, roughly half of Democrats seem to agree that cities should aim to reduce inequality. Democratic mayors also seem to favor redistribution to accomplish that goal. & Somewhat \\ \hline
%		Kiewiet, D Roderick
%		Mccubbins, Mathew D & 2014 & Annual Review & City budgets have been severeley constrained since the Great Recession. Spending has thus decreased in general. Lack of funds means that there is not much discretion for partisanship. & Somewhat \\ \hline
%		Tausanovitch, Chris
%		Warshaw, Christopher & 2014 & APSR* & Cities are responsive (taxes, expenditures, regressiveness of taxation) to citizens' conservatism/liberalism. Partisan elections do not make cities more or less responsive. & Yes \\ \hline
%		GuillamÃ³n, Ma Dolores
%		Bastida, Francisco
%		Benito, Bernardino & 2013 & European Journal of Law and Economics & Police spending in Spain. Conservative parties spend more on police. Spending is higher before elections. Also contains a useful overview of the literature. & Yes \\ \hline
%	\end{tabular}
%	\label{}
%\end{table}
%\end{landscape}
%
%\begin{landscape}
%	\begin{table}[htbp]
%		%\caption{}
%		\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%			\hline
%			Names & Year & Journal & Findings & Important? \\ \hline
%			Gerber, Elisabeth R. & 2013 & Cityscape & Partisanship of both citizens and elected city officials separately affect climate policy. & Yes \\ \hline
%			SolÃ©-OllÃ©, Albert
%			Viladecans-Marsal, Elisabet & 2013 & Journal of Urban Economics & Spanish cities. The authors "employ a regression discontinuity design to document that cities controlled by left-wing parties convert much less land from rural to urban uses than is the case in similar cities con- trolled by the right". Partisanship might also affect housing construction and price growth. & Yes \\ \hline
%			Gerber, Elisabeth R.
%			Hopkins, Daniel J. & 2011 & AJPS & Regression discontinuity design. Democratic mayors spend less on public safety. All other policy areas (including taxation) are unaffected. & Yes \\ \hline
%			Trounstine, Jessica & 2010 & Annual Review & Race and ethnicity in local elections (not relevant to us). Partisan elections have higher turnout; non-partisan elections still tend to have some partisanship in them because voters learn about party of candidates from media. Non-partisan elections favor Republicans/upper class. Mixed evidence for whether partisanship of mayor is important for policy. & Somewhat \\ \hline
%			Palus, Christine Kelleher & 2010 & State and Local Government Review & Ideology (liberal/conservative) of citizen is well represented by gov. spending in five areas: (1) community development, housing, and conservation, (2) health and human services, (3) culture, the arts, and recreation, (4) environmental programs, and (5) transportation. & Somewhat \\ \hline
%			Ferreira, Fernando
%			Gyourko, Joseph & 2009 & The Quarterly Journal of Economics & Regression discontinuity design. Null results for spending and city gov. size with regard to mayor partisanship. & Yes \\ \hline
%			Ansolabehere, Stephen
%			Snyder, James M. & 2006 & Scandinavian Journal of Economics & Despite the journal, this is about the U.S. The important finding (for us) is the fact that counties whose government is controlled by the same party as the state government, receive more funding (county's share of state transfers, normalized by county pop.) from the state. & Somewhat \\ \hline
%			Murphy, Russell D. & 2002 & Annual Review & Not useful. Too philosophical; mostly cites papers written a hundred years ago. Also exclusively about larger cities. & No \\ \hline
%		\end{tabular}
%		\label{}
%	\end{table}
%\end{landscape}
%
%\begin{landscape}
%	\begin{table}[htbp]
%		%\caption{}
%		\begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%			\hline
%			Names & Year & Journal & Findings & Important? \\ \hline
%			Armstrong, Cory L. & 2011 & Government Information Quarterly & Comparison of county and school board websites in Florida (where the two align) with regard to transparency (presence or absence of public records). Manual content analysis (undergrads told to look around for 15 minutes). School board websites, more professional websites, and websites in Republican-dominated counties are found to be more transparent. & Yes \\ \hline
%			Cegarra-Navarro, Juan
%			PachÃ³n, JosÃ©
%			Cegarra, JosÃ© & 2012 & International Journal of Information Management & Survey of Spanish municipal government officials (specifically, the city website managers). Respondents are asked about the features of their websites, the level of civic engagement and the size of their municipality. More sophisticated websites are correlated with greater civic engagement and greater use of e-government functions. & Yes \\ \hline
%			Dolson, Jordan
%			Young, Robert & 2012 & Canadian Journal of Urban Research & Determinants of website content. Three categories: e-content (city information on website), e-participation, social media use. Tables on page 15 show frequencies of these categories across sites, and might be useful to inform our topics. Larger cities have better websites. Population growth and immigration are also tested, but the findings are somewhat inconclusive. & Yes \\ \hline
%			Feeney, Mary K.
%			Brown, Adrian & 2017 & Government Information Quarterly & 500 U.S. city websites at two points in time (2010-2014). Count model of website features regarding information, e-services, utilities, transparency and civic engagement. Having a larger population leads to more features. Relying on a website contractor leads to more information and transparency. The authors say that mayor-councils are negatively correlated with website sophistication, but their regression tables state the opposite. & Yes \\ \hline
%			Kaylor, Charles
%			Deshazo, Randy
%			Van Eck, David & 2001 & Government Information Quarterly & Model of best practices of e-government. Table 1 lists a number of possible ways this manifests, could be useful for our theory. & Somewhat \\ \hline
%			Ansolabehere, Stephen
%			Urban, Florian & 2002 & Cities & Websites of 20 major cities across the world. Is website content correlated with city characteristics? Not particularly systematic, and the findings are inconclusive. & Somewhat \\ \hline
%			Jeffres, Leo W.
%			Lin, Carolyn A. & 2006 & Journal of Computer-Mediated Communication & 50 largest metropolitan areas in the U.S. Features include information about city, opportunities for citizen feedback, galleries of photos, links, etc.  Purely descriptive analysis, doesn't contain anything that isn't covered in any of the other aricles. & No \\ \hline
%		\end{tabular}
%		\label{}
%	\end{table}
%\end{landscape}
%
%\subsection{Informative Dirichlet model}
%%However, \cite{Monroe2008} advise against using these types of methods in this context because they get the data generation process backwards: Our theory assumes that party leads to variation in writing, and yet we rely on the documents to predict party, in spite of the fact that we actually have perfect knowledge of it.
%
%For the analysis of the data, we present two approaches, the first being the informative dirichlet model developed by \citep{Monroe2008}. This approach aims to account for the fact that some words naturally occur more than others by applying a Dirichlet prior based on the distribution of words in random text. Table \ref{tabFightinIN} shows the top words for both Democrats and Republicans - and accomplishes, to some extent, the goal of \citep{Monroe2008} of banishing frequent words from this list and supplanting them with text with greater semantic, and in our case, partisan meaning. 
%
%In Indiana, Democrats exhibit a preference for words related to public finance, such as 'fund', 'budget', or 'tax', indicative of a greater willingness to emphasize the city's efforts to raise and spend money. This finding is consistent with \citep{Einstein2015}, who show that Democratic mayors tend to favor greater spending. Beyond the focus on public finance, the words preferably used by Democrats do not fall into any particularly congruent categories, and largely sort into various areas related to city administration - i.e. `council', `services', `budget', `committee', `contract', etc. If there is theme around the words preferred by Republicans, it seems to center around city planning - street, fire, water, building, construction, park. These words suggest that the hands-off approach favored by Republicans results in a focus on supporting infrastructure and logistics.
%
%%Partisan top words - fightin words Indiana
%\input{tables/fightinwordsIN.tex} %\ref{tabFightinIN}
%
%For Lousiana, the results (see table \ref{tabFightinLA}) are less coherent. Only one of the finance-related terms appears again for Democrats - specifically `fund', although `rate' might also be used in a financial context. Beyond that, some focus on a `historic' `'district of a city seems evident, as is the use of some words - `infrastructure', `water', `building' that were used for Republicans in Indiana. Conversely, Republicans are now missing these words, and their preferred terms generally do not seem to follow any particular theme.
%
%%Partisan top words - fightin words Louisiana
%\input{tables/fightinwordsLA.tex} %\ref{tabFightinLA}
%
%The weakness of the fightin' words method is evident here, as a list of words does not necessarily provide sufficient information to glean preferred topics from. This is especially the case when the texts are spread across a broad number of issue-areas, with little semantic similarity. In \citep{Monroe2008}, the authors focus on the fairly constrained corpus of U.S. Senate speeches with respect to abortion - our context, by comparison, is far more eclectic.









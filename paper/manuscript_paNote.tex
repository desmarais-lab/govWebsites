\documentclass[11pt]{article}

%==============Packages & Commands==============
\usepackage{graphicx}
\usepackage{fancyvrb}
\usepackage{tikz}
%%%<
\usepackage{verbatim}
%\usepackage[active,tightpage]{preview}
%\PreviewEnvironment{tikzpicture}
%\setlength\PreviewBorder{5pt}%

\usepackage{geometry}                        % See geometry.pdf to learn the layout options. There are lots.
% \geometry{a4paper}                           % ... or a4paper or a5paper or ...
%\geometry{landscape}                        % Activat\usetikzlibrary{arrows}e for for rotated page geometry
%\usepackage[parfill]{parskip}            % Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}                % Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
                                % TeX will automatically convert eps --> pdf in pdflatex
\usepackage{amssymb}

\usepackage[ruled,vlined]{algorithm2e}
\usetikzlibrary{arrows}
\usepackage{alltt}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{indentfirst}
\usepackage[longnamesfirst]{natbib} % For references
\bibpunct{(}{)}{;}{a}{}{,} % Reference punctuation
\usepackage{changepage}
\usepackage{setspace}
\usepackage{booktabs} % For tables
\usepackage{rotating} % For sideways tables/figures
\usepackage{amsmath}
\usepackage{multirow}
\usepackage{color}
\usepackage{dcolumn}
\usepackage{comment}
\usepackage{pgf}
\usepackage{xcolor, colortbl}
\usepackage{array}

\def\mybar#1{%%
    #1 & {\color{red}\pgfmathsetlengthmacro\x{#1*0.006mm}\rule{\x}{4pt}}}

%\pgfmathsetlengthmacro\x{#1^0.1mm}
%\show\x -> 25.0pt

%\usepackage{fullwidth}
\newcolumntype{d}[1]{D{.}{\cdot}{#1}}
\newcolumntype{.}{D{.}{.}{-1}}
\newcolumntype{3}{D{.}{.}{3}}
\newcolumntype{4}{D{.}{.}{4}}
\newcolumntype{5}{D{.}{.}{5}}
\usepackage{float}
\usepackage[hyphens]{url}
%\usepackage[margin = 1.25in]{geometry}
%\usepackage[nolists,figuresfirst]{endfloat} % Figures and tables at the end
\usepackage{subfig}
\captionsetup[subfloat]{position = top, font = normalsize} % For sub-figure captions
\usepackage{fancyhdr}
%\makeatletter
%\def\url@leostyle{%
%  \@ifundefined{selectfont}{\def\UrlFont{\sf}}{\def\UrlFont{\small\ttfamily}}}
%\makeatother
%% Now actually use the newly defined style.
\urlstyle{same}
\usepackage{times}

\usepackage{lscape}
% \usepackage{mathptmx}
%\usepackage[colorlinks = true,
%                        bookmarksopen = true,
%                        pagebackref = true,
%                        linkcolor = black,
%                        citecolor = black,
%                     urlcolor = black]{hyperref}
%\usepackage[all]{hypcap}
%\urlstyle{same}
\newcommand{\fnote}[1]{\footnote{\normalsize{#1}}} % 12 pt, double spaced footnotes
\def\citeapos#1{\citeauthor{#1}'s (\citeyear{#1})}
\def\citeaposs#1{\citeauthor{#1}' (\citeyear{#1})}
\newcommand{\bm}[1]{\boldsymbol{#1}} %makes bold math symbols easier
\newcommand{\R}{\textsf{R}\space} %R in textsf font
\newcommand{\netinf}{\texttt{NetInf}\space} %R in textsf font
\newcommand{\iid}{i.i.d} %shorthand for iid
\newcommand{\cites}{{\bf \textcolor{red}{CITES}}} %shorthand for iid
%\usepackage[compact]{titlesec}
%\titlespacing{\section}{0pt}{*0}{*0}
%\titlespacing{\subsection}{0pt}{*0}{*0}
%\titlespacing{\subsubsection}{0pt}{*0}{*0}
%\setlength{\parskip}{0pt}
%\setlength{\parsep}{0pt}
%\setlength{\bibsep}{2pt}
%\renewcommand{\headrulewidth}{0pt}

%\renewcommand{\figureplace}{ % This places [Insert Table X here] and [Insert Figure Y here] in the text
%\begin{center}
%[Insert \figurename~\thepostfig\ here]
%\end{center}}
%\renewcommand{\tableplace}{%
%\begin{center}
%[Insert \tablename~\theposttbl\ here]
%\end{center}}

\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
\def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand{\N}{\mathcal{N}}
\newcommand{\Y}{\bm{\mathcal{Y}}}
\newcommand{\bZ}{\bm{Z}}

\usepackage[colorlinks = TRUE, urlcolor = black, linkcolor = black, citecolor = black, pdfstartview = FitV]{hyperref}


%============Article Title, Authors==================
\title{\vspace{-2cm} Government websites as data: \\ A methodological pipeline with application to the websites of municipalities in the United States}


\author{ Markus Neumann\footnote{Department of Political Science, The Pennsylvania State University, University Park, PA 16802, USA. Email: mvn5218@psu.edu. Corresponding author.} \and Fridolin Linder\footnote{Department of Political Science, Social Media and Political Participation Lab, New York University, New York, NY 10012, USA. Email: fridolin.linder@nyu.edu} \and Bruce Desmarais\footnote{Department of Political Science, The Pennsylvania State University, University Park, PA 16802, USA. Email: bdesmarais@psu.edu}} \date{\today}

%===================Startup=======================
\begin{document}
\maketitle



%=============Abstract & Keywords==================

\begin{abstract}

A local government's website is an important source of information about policies and procedures for residents, community stakeholders and scholars. Existing research in public administration, public policy, and political science has relied on manual methods of website content collection and processing, limiting the scale and scope of website content analysis. We develop a methodological pipeline that researchers can follow in order to gather, process, and analyze website content. Our approach, which represents a considerable improvement in scalability, involves downloading the entire contents of a website, extracting the text and discarding redundant information. We illustrate our methodological pipeline through the collection and analysis of a new and innovative dataset---the websites of over two hundred municipal governments in the United States. We build upon recent research that analyzes how variation in the partisan control of government relates to content made available on the government's website. Using a structural topic model, we find that cities with Democratic mayors provide more information on policy deliberation and crime control, whereas Republicans prioritize basic utilities and services such as water, electricity and fire safety.
%\noindent We explore the effect of transitions of power in municipal governments on the content of their websites. We hypothesize that when party control changes, city administrators modify the contents of their websites in order to fit the agenda of the new incumbent. To test this theory, we study cities in Indiana and Louisiana, two states in which all municipal elections are partisan and the parties of the candidates appear on the ballots. Snapshots of websites before and after transitions of power are acquired through the Wayback Machine. We apply statistical topic models based in latent dirichlet allocation, focusing on changes to the websites. We present results on both which topics see the greatest degree of change associated with transitions in city administrations, and how the topics modified differ with regard to political parties.

\end{abstract}
\thispagestyle{empty}
% \doublespacing
% Description of the possible challenges
\doublespacing

\noindent PA Letter Requirements:\\
2-4 pages\\
no longer than 1500-3000 words\\
1-3 small display items (figures, tables, or equations)\\
200-300 word abstract\\

\section{Introduction}
Local governments convey voluminous information about all aspects of their policymaking, policy implementation, and public deliberation, via their official websites. The vital role of official websites in connecting the government and the governed has motivated a wave of research on the contents of government websites \citep[e.g.,][]{grimmelikhuijsen2010transparency,wang2005evaluating,osman2014cobra,Eschenfelder2002}. The conventional approach to data collection in projects focused on government websites involves manual content extraction from each website in the dataset. Though accurate, the manual approach to data collection is costly for large-scale analysis. We present a methodological pipeline that can be used to automatically scrape government websites in order to build datasets that can be used for text analysis---describing challenges in data collection and processing, as well as the solutions we adopt. We provide an illustrative application in which we explore the ways in which the textual contents on city government websites in six American states correlate with the partisanship of the city mayor.

%Though there exists a variety of software tools that are designed to automatically scrape all of the files available at a website \citep{glez2013web}, raw website downloads have to be processed significantly before the files are adequately prepared for content (e.g., text, image) analysis. We describe and provide solutions to two central challenges in automatically gathering and analyzing website textual contents. First, plain text must be extracted from the files. This challenge would arise in any context in which researchers sought to study the textual contents of websites. The second challenge we address in our methodological pipeline is, however, specific to the research objective of comparing websites on the basis of a common lexicon. For any two governments, the textual signatures that most dramatically differentiate the textual contents of their websites consist of what we can call ``boilerplate'' text---header, footer, or other titling text that is designed to identify the website as being associated with a specific government entity (e.g., ``Welcome to the city of Santa Cruz'', ``The City of Los Angeles welcomes you''). The second methodological innovation we offer in our pipeline is designed to minimize the impact of this boilerplate text on the comparative analysis of government website content. 


\section{Mayoral Politics and City Government Website Content}


 A substantial body of research has found that the partisanship of the mayor affects city governance along multiple dimensions of spending and policy attention \citep{gerber2011mayors,de2016mayoral,einstein2016mayors,marion2013mayor}. Official city websites allow mayors to present their views and policy priorities to the public. In local politics, where campaign funds are low, this lends incumbents a crucial advantage in becoming more well-known among their constituencies \citep{stanyer2008elected}. Local government websites are frequently visited by the public \citep{thomas2003new}. City websites can be used to communicate the stance of a mayor on social or economic programs. 
 
%\begin{figure}
%\centering
%\includegraphics[scale=0.35]{figures/eriemayor}
%\caption{Screenshot from the homepage at \url{http://www.erie.pa.us/}, accessed on 06/14/2018. Image depicts  Democratic mayor of Erie, PA, Joseph Schember.}
%\label{fig:eriemayor}
%\end{figure}


%Members of the public visit municipal government websites for a wide variety of purposes \citep{sandoval2012government}, and with significant regularity. In a survey conducted among a random sample of citizens in the state of Georgia in 2000---nearly two decades ago---found that 25\% of internet users reported visiting a local government website in the previous twelve months \citep{thomas2003new}. %Furthermore, the use of a local government website is associated with an individual's perspective on government. \citet{tolbert2006effects} find that users of local government websites are more likely to trust local governments, and hold other positive attitudes related to local and federal governments. Lastly, in a study of residents of Kansas City, Missouri, \citet{ho2017government} find that participants' perceived quality of the city website is strongly associated with their perceptions of the overall effectiveness of the City's communication with the public.

The existing research that uses scraped websites provides an indication of the theoretical value of empirical analysis of web contents. Research on `e-governance' evaluates government websites in terms of accessibility, ease-of-use, and function \citep[e.g., ][]{Urban2002,mcnutt2010virtual,Armstrong2011,Feeney2017}. As an example, \cite{grimmelikhuijsen2012developing} study local government websites of Dutch municipalities to measure government transparency regarding air quality in the municipalities. The websites of politicians and their parties have also been the object of research \citep{Druckman2009,Druckman2010,Cryer2017,Esterling2011,Esterling2011a,Norris2003,Therriault2010}. For example, \cite{Druckman2010} analyze the issues engaged on websites for candidates in U.S. Congressional elections, and find that candidates strategically engage just a few issues based on the priorities in their districts and the characteristics of their opponents.  

%Important to our methodological objectives, research analyzing and improving the scraping, pre-processing and text analysis pipeline that is applicable to government websites is still in its infancy. \cite{Eschenfelder2002} provide something of an overview of how federal websites should be assessed from an e-governance point of view, but they largely focus on the substantive criteria that should be fulfilled, rather than the technical aspects of website acquisition and analysis. %In what follows we first define the target dataset---the textual contents of websites of United States municipalities, along with associated metadata on the municipalities and their governments. We then define a pipeline for data collection and analysis that includes methods to access government website URLs, scrape their raw contents from the World Wide Web, gather plain text from the website files, and identify boilerplate text within the plain text contents. Lastly, we illustrate the analysis of municipal government website text by exploring the relationship between the city mayors' party affiliations and the topical contents of the websites.



\section{Data: US Municipal Government Website Text}

For data availability reasons, we focus our analysis of municipal websites on six states---Indiana, Louisiana, New York, Washington, California, and Texas. The websites were scraped in March 2018. The selection of states and cities is largely dictated by the presence of partisan mayors and availability of the relevant data. Municipal elections in Indiana and Louisiana are partisan across the board, so our sample is primarily focused on these two states. For Indiana and Louisiana, all cities with a website are included, resulting in a considerably larger sample than for the other four states. New York and Washington do not have nominally partisan elections, but for a subset of cities, partisanship can be determined through contribution data (see appendix for more detail). California and Texas contain a number of large cities whose mayors are sufficiently well-known for their partisanship to be available. Our sample is well-balanced on a number of theoretically important dimensions. One, each of the four Census regions are represented with at least one state. Two, we have a fairly well-balanced sample with respect to the urban/rural cleavage. Furthermore, the sample is politically balanced---we have three blue states, and three red states.  The partisan breakdown of city websites is depicted in Table \ref{tab:cityparties}. Details on the sources and methods of raw data collection can be found in the Appendix.

\input{tables/tabCitiesStatesParties.tex} 

One of the more subtle aspects of local government is the presence of different types of government structures. Between council-manager governments and mayor-council governments \citep{morgan1992policy}---either in the weak or strong mayor variant \citep{desantis2002city}---there is variance in where a city's executive authority lies. We do not have access to information about the type of governments across the breadth of our dataset. Given the prominent place that mayors tend to have on their cities' websites, we feel that any bias arising from this nuance should be minor. \cite{gerber2011mayors}, whose theory is somewhat comparable to ours, find that the inclusion of this potential confounder does not affect the results.


\section{The Web to Text Pipeline}
In this section, we describe our methodological pipeline, with which we take an archive of website files, and output a corpus of formatted plain text documents that are suitable for comparative analysis with text as data methods. Here, we address three methodological challenges.  First, though they contain significant amounts of text, websites are not comprised of clean plain text files. Rather, the files available at websites are of multiple types, including HTML, PDF, word processor, plain text, and image files. The first step is aimed at extracting clean plain text from this heterogeneous file base. The second step in our pipeline is to process the text to remove language that is effective at differentiating one website from another but is uninformative regarding policy or political differences between governments. Finally, these tools need to work consistently across all of the websites in our corpus, in spite of the fact that relevant information is stored and structured in different ways.%Some of the steps we take in this processing pipeline are universally applicable in the analysis of textual data, and some of them are most appropriate for the particular type of text analysis that we apply to this data---statistical topic modeling. We will clarify this distinction as we describe steps in our pipeline.

\subsection{Site to Text Conversion}
\subsubsection{File Type Detection}
The format of a file has a major impact on whether and how textual data can be extracted from a document. For the most part, the file type of a document can be correctly determined through the filename ending---its extension. However, there are exceptions to this, which, if ignored, can lead to large amounts of improperly formatted text, arising from incorrectly converted documents, which leads to a general decrease in the amount of usable data. Two issues, in particular, need to be addressed: One, HTML files on city websites frequently do not have an ending but are still perfectly readable if correctly identified as such. Second, some documents contain the incorrect file ending. For example, we found thousands of documents that ended in .html, when they were actually PDFs. To accurately assess their type, we rely on the R package \texttt{wand} \citep{wand}, which is an R interface to the Unix library \texttt{libmagic} \citep{darwin2008libmagic}, which determines the type of a file on the basis of its file signature - or ``magic number''. This short sequence of bytes at the start (and sometimes end) of files is unique for each file type and therefore allows its correct identification through computer forensics tools such as \texttt{libmagic}. Files with incorrect endings are subsequently renamed.

\subsubsection{Extracting Text from HTML}
The HTML files that websites are comprised of contain a large amount of useful information, but also completely irrelevant text such as menus, navigational elements and other boilerplate. In theory, HTML tags intended to mark text content  exist to prevent this problem. In practice, not every web developer adheres to this standard, and even when they do, there is a large degree of variation in the way it is implemented. Parsing any one website is fairly straightforward, but parsing the 283 different websites in our sample in a consistent and comparable manner makes a rule-based approach impossible.

We leverage methods developed in the information retrieval literature to deal with this problem. These boilerplate detection tools are classifiers which rely on both structural features, such as HTML tags, as well as text statistics such as word and sentence length to estimate whether a given portion of an HTML file is useful. We rely on the method described in \cite{Kohlschutter2010}, which has been published in a reputable conference, is widely cited and can easily be implemented through the R package \texttt{boilerpipeR}.

\subsubsection{Extracting Text from PDF, XML, DOC, DOCX and TXT}
Other files are read in through the \texttt{readtext} R package \citep{readtext}, which is a wrapper for a set of parsers.\footnote{\texttt{readtext} determines a document's type solely through its ending -- so the conversion described above is strictly necessary for the package to work correctly.}$^{,}$\footnote{We have also experimented with several Unix-based alternatives, but found that they largely led to the same results as \texttt{readtext}.}$^{,}$\footnote{\texttt{readtext} also contains an HTML parser, but this tool simply extracts all text and is therefore inferior to boilerpipe.} The breakdown of all files by type is given in Table \ref{tab:filetypes}. The most frequent file type besides HTML is PDF, from which we are able to extract a substantial amount of usable text. Files of type XML, DOC, TXT, and DOCX, also occur regularly in our corpus and offer a considerable volume of textual data.

\input{tables/tabFiletypeFrequencies.tex}

\subsection{Preprocessing}
Preprocessing is an important part of text-as-data research and choices made therein can have significant effects on the outcomes of an analysis \citep{denny2018text}. The challenge in conducting preprocessing for a comparative analysis of websites lies in the considerable variance between websites. Some of it is substantively informative and some of it is completely irrelevant. As an example of the latter, names of city officials and citizen petitioners feature frequently in city documents. The same is true for streets, locations and not least of all, the city itself. Since individual names recur at a much higher rate within a city than across the entire corpus, this would cause a topic model to cluster its topics by city. Consequently we require a tool which detects the signal in the noise and does so consistently for a discordant set of sources.

%The type of text analysis we conduct---topic modeling---requires that the words in the document be meaningful and interpretable, and does not make use of the sequence of words within a document (i.e., is a bag-of-words method). %With the exception of file type conversion and boilerplate removal, the pre-processing steps taken here should therefore not be seen as universally applicable to all analyses of government website data.

To this end, we turn to to an approach that is much more common in natural langauge processing than political methodology -- part-of-speech (POS) tagging and named entity recognition (NER). While not their original purpose, these procedures perform very well in separating the wheat from the chaff. As names convey no substantive information, we NER is used to detect to remove them.\footnote{We retain laws, nationalities or religious or political groups (which are politically salient with respect to immigration and identity) as well as works of art (which frequently feature statutes, plans, etc.)} Furthermore, we select words on the basis of their POS-tags, retaining only nouns (by far the most informative category), verbs, adjectives. Furthermore, we keep proper nouns that also occur as nouns -- this removes names, but retains titles such as ``Police Chief'' which can appear as proper nouns if they are followed by a name. Finally, we also conduct lemmatization to reduce words to their basic form.\footnote{Lemmatization is similar to stemming, but works in a somewhat more sophisticated manner by taking grammar and surrounding words into account to identify the dictionary form of a word. For example, the lemma of the word ``lemmatization'' would be ``lemmatize'', whereas most stemmers would simply chop off the ending, which would yield ``lemmatiz''. Thus, lemmatization makes the results more easily comprehensible.} POS-tagging, NER and lemmatization are all implemented through \texttt{spacyR}. This parsing-based approach is very effective in distilling a comparable corpus from a varied set of sources, as the grammatical rules of the English language remain their common denominator. To deal with any leftover issues, we remove words with less than three characters (these are usually artifacts from improperly encoded documents and faulty or impartial optical character recognition), stopwords and non-English words (using the R package \texttt{Hunspell})\footnote{As we are using \texttt{spacyR} with an English parser, non-English words tend to be erroneously categorized as (proper) nouns.}.

A final and crucial step is the removal of duplicate documents, which occur very frequently on websites. In addition to their primary purpose, the previous preprocessing steps also help in stripping otherwise identical documents of information that makes them unique -- such as names and dates -- thus facilitating their deletion. The removal of duplicate information also solves a problem posed by a recurring feature of municipal websites -- city calendars. For most days in most cities, these calendars tend to be empty, but nevertheless contain a template, which would lead to the creation of a document in our corpus. These short and repetitive -- but numerous -- texts would pose severe problems to a topic model. Our approach preserves the valuable information contained within these calendars when cities do make use of them, but removes them when they offer no substantive data.

After all the preprocessing is set and done, our corpus consists of 259,099 documents.

%The text documents are converted to UTF-8 character encoding and then stripped of dates, punctuation, numbers, and words connected by underscores. At this point, the documents of one city still closely resemble one another in the form of boilerplate content, be it website elements (i.e. "You are here", "Home", "Directory" etc.) in HTML documents, or commonly used forms or phrases in pdf, doc and docx files. This is an issue, because this boilerplate content causes the results of analyzing this data with text analysis methods to characterize documents primarily by the cities from which they originate (through their unique boilerplate structure, e.g. a menu with certain terms repeated on every site of the domain), and not the substantive features of their contents. Boilerplate removal is a useful step in many forms of text analysis, as the analysis is focused in on text that varies above and beyond a standard template for textual content. Our solution to this problem is described in more detail in Section \ref{boilerplate}.

%The last round of preprocessing is intended to remove everything from the file that is not an English word. This step is tailored to our intention to use the text for topic modeling. The final preprocessing round includes setting every character to lowercase, as well as the removal of bullet points which frequently occur in HTML documents, extraneous whitespace, XML documents mislabeled as HTML files, and empty documents. Furthermore, some documents contain gibberish, often as a result of faulty or impartial optical character recognition applied to text that was produced through a non-machine-readable medium. To combat this problem, we employ two solutions. One, we use spellchecking, implemented through the \texttt{hunspell} R package \citep{hunspell}, to remove all non-English words.\footnote{Some of the cities, for example, Los Angeles, do contain a sizable proportion of Spanish content. The analysis of this content is beyond the scope of this paper but could be explored in future work, for example using methods of text processing that are applicable to multilingual corpora \citep{lucas2015computer}. } However, \texttt{hunspell} does not cover everything, either because some tokens are not actual words (for example artifacts from defective encoding), or because random sequences of characters just so happen to form words that exist in a dictionary (for example "eh" or "duh"). Since we rely on a bag-of-words model in which syntax does not matter, we can ameliorate these problems by removing all text except for whitespaces and the characters that appear in the English alphabet. Since a lot of the nonsensical text tends to be quite repetitive, we also delete all documents in which the proportion of unique to the total number of tokens is less than 0.15. Furthermore, \texttt{hunspell} does not spellcheck individual characters or two-character words, so we remove these token types entirely. Since these pre-processing steps reduce documents which are largely unsuitable to only a few tokens (i.e., word occurrences), we also remove all remaining documents containing less than 50 tokens. Finally, to remove words that are extremely rare (which also has the advantage of eliminating any remaining oddities) and thus add nothing substantive\footnote{Topic models essentially do not pick up on extremely rare words, so their inclusion is a waste of computational resources. Removing them in this manner is also the default preprocessing choice in the \texttt{stm} package.} to our models while increasing their computational cost, we also discard any token types that occur in only one document.

%\subsection{Boilerplate Removal}\label{boilerplate}
%As noted above, city websites contain a large amount of text that is uninformative for its actual content, and therefore a hindrance to understanding through algorithmic text analysis. This is a common issue with textual data in which informative content is embedded in technically structured documents. See, e.g., \citet{burgess2016legislative,wilkerson2015tracing} and \citet{linder2018text} for examples of boilerplate removal in the analysis of legislative text.

%\subsection*{Boilerplate Classification}
%In order to determine whether a line should be discarded, we train a classifier on a human-coded sample. We sampled 500 lines from documents in each of the following five cities: Los Angeles, CA, Indianapolis, IN, New York, NY, Shreveport, LA, and Seattle, WA. To ensure that lines which occur more frequently in these cities (sometimes hundreds of thousands of times) had a higher probability of being scrutinized by the classifier, we use sampling weights equivalent to the proportion of total lines in a city's corpus made up by each specific line type. As an example, the most common line throughout all pages of the city of Seattle consists only of the word ``total'' and occurs 103,068 times. Similarly, the line ``page'' occurs 58,833 times. Even something completely nonsensical such as ``a a'' still appears on 376 occasions. To account for the higher likelihood of some lines being part of the training set, we use inverse probability weights in training the classifier---the weight of each line in the sample is 1/[number of occurrences in the corpus].\footnote{Note that the performance of the classifier is robust to the use of these weights and only changes by about one percentage point if they are not used.}

%These 500 lines were then hand-coded as either substantively informative (210 lines) or not (290 lines). We then trained a number of different classifiers with this informativeness measure as the dependent variable. The independent variables we use are: (1) number of times the line was duplicated within the city, (2) the length of the line, in characters, (3) the number of tokens in the line, and (4) the median distance from the document midpoint to the position of the line itself. The purpose of these covariates is as follows:

\section{Partisan Language on Municipal Websites}

We illustrate the analysis of municipal website content by studying differences in website content based on the party of the mayor. As we reviewed above, the partisanship of the mayor has been found in past research to affect several features of city governance. However, \citet{gerber2011mayors} note that, due to the constraints of state and national policies, municipalities lack discretion in many domains of governance. These constraints do not apply to website contents. City governments have great discretion in composing their websites, modifying website content is low cost relative to other policy changes, and, as reviewed above, city websites provide an effective and often-used means of communication with city residents. 

%BoW methods are methods of text analysis that do not take into account the sequence or placement of words in text---just the presence and frequency of words. As noted by \cite{GrimmerStewart2013}, for most applications, bag-of-words approaches have been found to be more than sufficient. Furthermore, there is reason to believe that city government websites are a particularly `safe' case for bag-of-words methods due to their informative, manner-of-fact based language. It is extremely unlikely for these pages to feature ambiguous language such as an abundance of negation or even sarcasm.


%\input{tables/tabBoilerplateIllustration.tex}

In order to analyze content differences between government websites based on mayoral partisanship, we draw upon a recently-developed class model for text, the structural topic model (STM), developed by \citet{Roberts2014}. Building on the conception of ``topics'' in Latent Dirichlet Allocation, in the STM a topic is a multinomial distribution defined on the word types in the corpus dictionary. The log-odds of the topic probabilities in each document-specific multinomial distribution over topics are drawn from a multivariate normal distribution in which the topic-specific means are determined by a linear regression function that associates document-attributed covariates with topics. For example, in the context of municipal website content, the structural topic model can be used to estimate a regression coefficient that defines the linear relationship between the log-odds of the municipality's population and the log-odds of each topic. For our primary empirical investigation, the STM provides with a tool with which to estimate the relationship between the party of the city's mayor and the prevalence of each topic we estimate. Further details on our STM specification can be found in the appendix.


\subsubsection{Structural topic model results}


The results are shown in Table \ref{tabSTMtopwords60}. Many of the topics associated with Democrats fit with what we understand to be national party priorities. Topic \textbf{52}, on affordable housing, clearly resonates with the Democratic party's appeal to low-income voters. Similarly, employee rights are represented in topics \textbf{10} and \textbf{29} \textbf{[DEBATABLE]}. Democrats also exhibit a strong preference for words related to public finances, such as Topic \textbf{58} ('budget', 'revenue', 'expenditure'), \textbf{topic 45 ('asset', 'actuarial', 'liability', 'financial')}, \textbf{topic 35 ('bond', 'obligation', 'proceeds')} as well as \textbf{topic 55 ('taxable', 'deed', 'value')}. We suspect that the association of Democratic mayors with finance-related terms is indicative of a greater willingness to emphasize the city's efforts to raise and spend money. This finding is consistent with \citep{Einstein2015}, who show that Democratic mayors tend to favor greater spending. A second, consistent Democratic focus appears to be law enforcement: The most Democratic topic, \textbf{59} ('burglary', 'robbery', 'theft', 'homicide') depicts Democrats' complicated relationship with law enforcement \textbf{[This used to be about police as well, but now it's more of a crime topic]}. On the one hand, Democratic partisans have a more negative perception of the police, rating it considerably more negatively on the appropriate use of force and the equal treatment of minorities \citep{Brown2017}. On the other hand, the literature has also shown that cities with a higher Democratic vote share spend more on the police, even after controlling for crime \citep{Einstein2015}. Finally, Democrats also focus more on the deliberative process of policymaking, as \underline{topics 31 ('agenda', 'committee') \textbf{[This is now the second-most REPUBLICAN topic 46]}}, \underline{34 (`comment', `draft', `feedback') \textbf{[This is now REPUBLICAN topic 5]}}, \textbf{1} (`absent', `preside', `authorized'), and \textbf{4} ('audit', 'procedure', 'oversight') attest to. \textbf{[Republicans also have topic 16]}  This openness regarding the policy process on behalf of cities with Democratic mayors fits with the findings of \citet{grimmelikhuijsen2012developing}, which are that left-wing local governments exhibit greater transparency via website content.

City websites with Republican mayors, meanwhile, exhibit a pronounced focus on the essential functions of government. Basic utilities such as energy (Topic \textbf{20}), fire protection (Topic \textbf{51} \textbf{[This topic has changed a lot.]}), \underline{drinking water (53) \textbf{[topic missing]}}, and \underline{garbage removal (Topic 49) \textbf{[topic missing]}} are included among those topics that are more prevalent in cities with Democratic mayors. Similarly, protecting citizens from natural disasters is a focus in \underline{topics 1 ('storm', 'runoff', 'drainage') \textbf{[topic missing]}} and \textbf{2} ('influenza', 'infection', 'vaccine' \textbf{[This is not about Zika anymore, but infectious diseases in general.]}), which may reflect the greater prevalence of Republican mayors in the southeast, a region which is more often affected by hurricanes and tropical diseases.



%Interestingly enough, Democrats also `own' the topic related to law enforcement, which might be somewhat unexpected given Republicans' usual focus on law and order \citep{gerber2011mayors}. However, this kind of finding is not entirely without precedent in the literature (see \citep{Einstein2015}). Similar to the informed dirichlet model, the structural topic model also finds the emphasis on construction and infrastructure by Republicans - in table \ref{tabSTMINRep}, topics 2, 7 and 8 clearly focus on these issues.\footnote{The first Republican topic in Indiana (library, stream, obj, etc.) is likely an artifact from incorrectly converted HTML, and since it presumably only happens only in one Republican city, the topic is classified as very Republican.}

%When comparing Indiana to Louisiana, it appears that the Democratic emphasis on law enforcement is robust. Furthermore, as with the fightin' words approach, some smaller degree of focus on money (see topic 1) is still evident. For Republicans, topics 2 to 4 seem to be, once again about infrastructure and utilities, pointing to a certain degree of robustness in these results, as well as the emergence of a trend. The results produced by the structural topic model are not flawless, but the two parties do seem to have somewhat consistent themes on which they focus on in both states. Furthermore, in comparison to the fightin' words approach, the ability of the structural topic model to form coherent topics is quite evident and helpful in the interpretation of the results.

%An ostensibly intuitive solution to topics clustering into cities in LDA is to include dummies for the cities in a statistical model of topics. This is facilitated by the structural topic model, which uses document metadata t to account for variation in topics \citep{Roberts2014}. However, figure \ref{stm_results} shows that if anything, the STM exacerbates the problem. Here, we plot the \textit{p-values} of the coefficients for each city as well as the party variable across each topic. Under normal circumstances, plotting the p-values, as opposed to the fitted values, does not make much sense, but here it serves a diagnostic purpose. The plot shows that the party variable is never statistically significant at any conceivable level of confidence, nor is it even close to. Interestingly the same is true for a number of the cities as well. The topics cluster heavily into only about half of the cities, which does not present an improvement over LDA at all.

%Partisan top words - stm Louisiana -- Rep
%\input{tables/stmTopWordsINRep.tex} %\ref{tabSTMINRep}

%Partisan top words - stm Louisiana -- Dem
%\input{tables/stmTopWordsINDem.tex} %\ref{tabSTMLADem}

%Partisan top words - stm Louisiana -- Rep
%\input{tables/stmTopWordsLARep.tex} %\ref{tabSTMLARep}

%Partisan top words - stm Louisiana -- Dem
%\input{tables/stmTopWordsLADem.tex} %\ref{tabSTMLADem}

%\subsubsection{Prediction with SVM}
%An alternative approach to the problem is to ignore topics entirely and go straight to predicting documents that are much more likely to be included on websites belonging to one or the other party. Classic machine learning techniques such as Naive Bayes, Linear Discriminant Analysis, or SVM should be expected to fare well in this context. Here, we rely on SVM, implemented with the SciKitLearn package in Python.\footnote{We also implemented SVM in R through the packages kernlab and e1071 in R. However, neither of these provide a regularized version of SVM (NOTE: at least that is what I am gathering from the stack overflow error), which prevents us from using all of the features contained in our data. Instead, we ranked the features according to tf-idf and selected the top 5000. These methods are also quite slow, and provide a maximum accuracy of 82\% in five-fold cross-validation.} A grid search reveals the tf-idf representation of the document-term matrix to be better than pure word counts, unigrams to be superior to bi-grams, the application of an L2 penalty to be preferable to either L1 or elasticnet, and an alpha (a constant to multiply with the regularization parameter C) of 0.0005 to lead to the best results. Applying five-fold cross-validation to the (tf-idf) document-term matrix with the dimensions 16011x35000 leads to an average accuracy of 89\%.\footnote{Other methods used: Elastic-net in the glmnet package in R. Accuracy is 0.6924795 for in-sample prediction, so not worth bothering with.}

%However, \cite{Monroe2008} advise against using these types of methods in this context because they get the data generation process backward: Our theory assumes that party leads to variation in writing, and yet we rely on the documents to predict party, in spite of the fact that we actually have perfect knowledge of it.



%Heatmaps
%\begin{figure}[htp]
%    \centering % Using \begin{figure*} makes the figure take up the entire width of the page
%    \caption{Word-topic probabilities for topics with big partisan differences, across documents (Indiana).}
%    \label{heatmaps_weights}
%    \includegraphics[width=0.8\linewidth]{figures/heatmaps_weights_IN.png}
%\end{figure}



%US map
%\begin{figure}[htp]
  %  \centering % Using \begin{figure*} makes the figure take up the entire width of the page
    %\includegraphics[width=\linewidth]{figures/us_map.pdf} \vspace{-2cm}
       % \caption{Map of the cities in the corpus in the contiguous U.S. The corpus also includes Alaska. In the analysis, only cities in California, Indiana, Louisiana, New York, Texas and Washington are used. The colors represent the partisanship of the mayor (blue corresponding to Democrats and red to Republicans).}
    %\label{us_map}
%\end{figure}

%stm results
%\begin{figure}[htp]
%    \centering % Using \begin{figure*} makes the figure take up the entire width of the page
%    \caption{Results from a structural topic model, displayed as the p-values for each variable for each topic. This would normally be somewhat nonsensical, but here it illustrates why the model does not work.}
%    \label{stm_results}
%%    \includegraphics[width=1.1\linewidth]{figures/stm_results.pdf}
%\end{figure}



%Partisan top words - topic model Indiana
%\input{tables/partisanTopWords.tex} %\ref{tabFightinIN}

%Partisan top words - topic model Louisiana
%\input{tables/partisanTopWordsLA.tex} %tabLDALA

%Partisan top words - stm Indiana
%\input{tables/stmTopWordsIN.tex} %\ref{tabSTMLA}

%Partisan top words - stm Louisiana
%\input{tables/stmTopWordsLA.tex} %\ref{tabSTMLA}


%Some basic descriptive statistics of documents by party
%\input{tables/descriptiveStatisticsPartisanIN.tex}

%Some basic descriptive statistics of documents by party
%\input{tables/descriptiveStatisticsPartisanLA.tex}

%fightin words results
%\begin{figure}[htp]
%    \centering % Using \begin{figure*} makes the figure take up the entire width of the page
%    \includegraphics[width=\linewidth]{figures/linesCutoffIN.pdf}
%        \caption{Total number of lines retained at a given threshold for removing duplicated lines. For example, at x = 10, all lines occurring more than 10 times within a city's documents are removed.}
%    \label{linesCutoff}
%\end{figure}


%\section{Ground truth test}
%In the realm of public administration, the notion that the partisan leaning of mayors might have an effect on how they run their cities is still frowned upon to some extent. Perceived more as managers than politicians, they have been portrayed as the last bastion of non-partisanship in America, and in many cases, also style themselves that way \citep{Dovere2018}. However, the aspirations some mayors have shown towards higher offices - in some cases, even the presidency - reveal that they are not quite as above the fray as some may believe them to be. One of the most vicious and blatantly partisan cleavages in current U.S. politics - the debate surrounding sanctuary cities - has seen mayors in a central role. Research into local politics has shown that partisan elections consistently have greater turnout \citep{Schaffner2001}. When voters are denied this cue, they make use of other, and considerably more irrational heuristics, such as name, gender, or occupation of the contenders. Consequently, it only makes sense for any office-seeking politician to emphasize their partisanship. Finally, decades of research in political psychology have consistently shown that no matter how hard we try, humans are simply incapable of escaping our partisan biases, a finding that is especially pronounced among elites \citep{Hatemi2011}.
%
%In an effort to underline this fact and remove any doubt about the fact that the partisanship of mayors colors their decision-making, we conduct a ground truth test between our main corpus - the websites of cities - and a second, decidedly more partisan set of texts: the campaign websites of these mayors. As noted above, partisanship has been shown to be a powerful driving force even in local politics, and mayors are incentivized to exploit it. Consequently, they are very likely to emphasize conservative/liberal values on this platform. If there is a greater correlation in word use between the cities managed by a party and the campaign websites of its mayors than with those of the other party, evidence for the partisanship of city websites can be established.
%
%Using the same methods as described for our main corpus, we have gathered these sites and then concatenated all of the documents belonging to mayors of the two parties into one ground truth document each. We do the same for the city documents, and then compare the four document collections using cosine similarity. This measure is the cosine between the angle of two vectors, in this case, the frequencies of all words in the two vocabularies. Compared to a simple Euclidean distance, this has the advantage of accounting for the fact that the two corpora being compared are not necessarily of the same length. The cosine measure between two documents ranges between 0 and 1, 0 signifying absolutely no correlation, and 1 perfect overlap. Figure \ref{groundtruth} shows the result of this test. The expectation is for a greater similarity between Republican cities and the Republican ground truth than Republican cities and Democratic ground truth - and vice versa. At present, however, this does not appear to be the case, presumably because the Republican ground truth consists of 8 documents, and the Democratic one of 290.

%ground truth test
%\begin{figure}[htp]
%    \centering % Using \begin{figure*} makes the figure take up the entire width of the page
%    \includegraphics[width=\linewidth]{figures/groundtruth_corrplot.png}
%    \caption{Ground truth test. The values are cosine similarities between a pair of document collections.}
%    \label{groundtruth}
%\end{figure}
%
%%ground truth test
%\begin{figure}[htp]
%    \centering % Using \begin{figure*} makes the figure take up the entire width of the page
%    \includegraphics[width=\linewidth]{figures/groundtruth_bs_bigcities_corrplot.png}
%    \caption{Ground truth test. The values are cosine similarities between a pair of document collections (top 100 mayors vs. IN and LA).}
%    \label{groundtruth}
%\end{figure}

%Ground truth test between top100 mayors and IN + LA; with bootstrapped confidence bounds
% label: groundtruth_bootstrapped
%\input{tables/groundtruth_bootstrapped.tex}


%\input{tables/stmTopWords.tex} %\ref{tabSTMtopwords}


\input{tables/stmTopWords60.tex}
\input{tables/stmTopWords60_OLD.tex} %\ref{tabSTMtopwords2}

\section{Conclusion}


We have developed a methodological pipeline for automatically gathering and preparing government websites for comparative content analysis. This methodology holds the potential to vastly scale up the data collection efforts underpinning the growing body of research that is focused on government website analysis. Through an application to the analysis of municipal websites in six different states, we show how our pipeline is capable of gathering corpora that shed light on the forms and functions of local government. We find that government website contents are associated with the partisanship of the mayor in ways that would be expected based on the parties' national priorities and past research on the effects of mayoral partisanship on city governments.

%We offer several contributions that will be valuable in future research endeavors. First, the data collected in the current study can be used for comparative analysis of US city website contents. Second, the pipeline we present can be used as a set of procedures to follow in gathering large-scale datasets of textual contents from other samples of governments. For example, the pipeline we have developed could be used to build comparative datasets of state or federal bureaucratic agencies. Third, our findings regarding the effects of mayoral partisanship on city website contents advance the literature on the role of partisan leadership in local government, and reinforce the finding of \citet{gerber2011mayors} that the effects of mayoral partisanship can be best observed through the analysis of domains of government (e.g., website contents) that are not heavily constrained by state or national governments.

\section*{Funding}
This work was supported by the National Science Foundation [1320219, 1637089, 1641047].

%\input{tables/tabCitiesStatesParties.tex}
%\input{tables/stateUrlSummary.tex}

\newpage

\bibliographystyle{apsr} % apsr stopped working for me
%\bibliographystyle{plainnat}
\bibliography{ref}

\newpage
\section*{Appendix}

\subsection*{Raw data collection methods and sources}

We acquired the website URLs from two sources: One, we scraped the URLs of city websites from their respective Wikipedia pages, which we found from lists of cities contained within each state. Two, the General Services Administration (GSA) maintains all `.gov' addresses, and provides a complete list of all such domains to the public.\footnote{The dataset is made available at \url{https://github.com/GSA/data/tree/gh-pages/dotgov-domains}. This list is updated once per month---we rely on the version released on January 16, 2017.} The data from the GSA contains the following variables: (1) domain name, specifically, the all-uppercase version of domain and top-level domain (for example, 'ABERDEENMD.GOV'); (2) the type of government entity to which the domain is registered, such as city, county, federal agency, etc; (3) for federal agencies, the name is specified; (4) the city in which the domain is registered. Naturally, the GSA's list does not contain cities which do not use a `.gov' website (or, in many cases, a city owns a registered `.gov' address, but uses a different one). Furthermore, some of the links are non-functional, and some of the county websites on the list are incorrectly marked as city websites (and vice versa). Since the GSA data is less complete and less reliable than the URLs found on Wikipedia, we mainly rely on the latter and only supplement them with the GSA data if a specific city doesn't have a URL recorded on Wikipedia, or our tests (see below) find it to be non-functional.

Not all of the URLs contained in these archives are functional. To test the URLs' functionality, we use a web driver-controlled browser - a browser that is automatically controlled by a program rather than a human user. We use the Python bindings for the program \texttt{Selenium}, which we use to control \texttt{Firefox} through the web driver  \texttt{Geckodriver}. This is advantageous compared to conventional scraping tools such as \texttt{Beautiful Soup} or \texttt{Rvest} because most websites are designed to be explored by browsers. Modern browsers perform a lot of actions behind the scenes, such as URL resolution and redirection. The use of a web driver-controlled browser is necessary in our case because a) some city websites simply don't work, but they don't always output an error code correctly (this can fail, for example, if a webmaster simply stops maintaining a site without removing it entirely) which would throw off an automatic scraper, and more often, b) cities sometimes change their websites' URLs, in which case they redirect from the old to the new URL. A web driver-controlled browser, unlike the more rigid conventional scraping tools, will simply follow this redirection. This allows us to subsequently record and use the new URL for the actual website scraping. Consequently, an automated browser allows us to robustly answer the following questions: Is the website actually there? Does it work? If not, is it somewhere else or is it broken? We record this information and construct a list of verified URLs.

To download the websites, we rely on the Unix command line tool \texttt{wget}. This program is used to download files from the Internet, and with the use of a recursive option, acts like a web crawler and scraper. This means that \texttt{wget} downloads HTML files, parses them and then follows the links contained therein. Then it follows those links and repeats the process until it has constructed a complete tree of the website (note that the program is instructed to stay on the same domain, i.e. it does not follow external links). This way, all the files that make up a website are downloaded. For some cities, whose websites make heavy use of JavaScript to serve content dynamically, such content is not reachable with our methodology and would require additional steps to obtain. For this paper, we ignore such sites and restricted our corpus to cities with at least three successfully downloaded pages.\footnote{There is a possibility that this leads to a small bias in selecting against cities with the resources to build more elaborate websites. However, given that our sample is generally more on the wealthy side, this, if anything, should lead to a more balanced sample.}

The partisanship of the mayor of each city is coded in different ways, depending on the state. For Indiana, where elections are nominally partisan, this information is accessible through the state government's website\footnote{\url{http://www.in.gov/apps/sos/election/general/general2015?page=office&countyID=1&officeID=32&districtID=-1&candidate=}}. For Louisiana, we received data on the outcomes of mayoral elections from the Local Elections in America Project (LEAP) \citep{marschall2013local}. For the other states, where mayoral elections are not nominally partisan (but the partisanship of the mayor is still well-known), we employed different means: For New York and Washington, we searched the state campaign finance websites, and coded the parties of the candidates based on the party committees from which they received donations. For California and Texas, where our data consists of highly populated cities, partisanship information was acquired from Ballotpedia\footnote{\url{https://ballotpedia.org/List_of_current_mayors_of_the_top_100_cities_in_the_United_States}}. Finally, we also scraped mayoral partisanship from the cities' Wikipedia pages. When compared to the other data sources above, (and manual searches in case of conflicts) Wikipedia proved to be very reliable and added additional cases to our dataset even for Indiana and Louisiana. Generally speaking, we found data scraped from Wikipedia, aided by manual corrections in case of missing or conflicting data, to be more reliable than data from governmental sources.\footnote{In Indiana, the data includes only cities - incorporated municipalities with at least 2,000 inhabitants - as opposed to towns.}


Information on other covariates (population and median household income - from the American Community Survey 5-Year Data (2015)) was acquired through the API of the U.S. Census Bureau\footnote{\url{https://www.census.gov/data/developers/data-sets.html}}.


\section*{Details on STM specification}

The structural topic model is implemented in the R package \texttt{STM} \citep{stm}. We use 60 topics---the number recommended by the authors\footnote{For this recommendation, see the documentation for the function \texttt{stm()} in version 1.3.0 of the R package \texttt{stm} \citep{stm}.} for medium- to large-sized corpora.\footnote{Since our corpus is at the larger end of that spectrum, we also estimated a model with 120 topics, but found no notable differences.} We use four covariates: First, \textit{party}, to estimate the difference in topic prevalence based on whether mayors are Republican or Democratic. Second, \textit{city population}, which the literature frequently emphasizes as a determinant of the issues a city faces (see, for example, \cite{Guillamon2013}). Third, we control for wealth by relying on \textit{median income} as a covariate, which we use as a proxy for the tax base in a city. Fourth and finally, we include state dummy variables, which should account for language that is associated with state-specific issues, and general background variables that vary across states.\footnote{The ``Fightin' Words'' methodology developed by \citet{Monroe2008} could also be used to analyze word-frequency differences between cities based on mayors' partisanship, but we elected to use the structural topic model since, unlike ``Fightin' Words'' , the structural topic model enables us to adjust for several other features through multiple regression.} 


\input{tables/stmTopWords120_1.tex}
\input{tables/stmTopWords120_2.tex}


%\begin{figure}[htp]
  %  \centering
   % \caption{Five largest topic effects for the population covariate. The fact that the population and epidemiology topics are positively correlated with city size is indicative of the model's validity.}
    %  \label{stmEffectPop}
    % \includegraphics[width=\linewidth]{figures/stm_effect_pop.pdf}
% \end{figure}

% \begin{figure}[htp]
  %  \centering
   % \caption{Five largest topic effects for the median income covariate. The fact that the crime topic is most prevalent in poorer cities, good governance is the most positively correlated with income is indicative of the model's validity.}
   % \label{stmEffectIncome}
    %\includegraphics[width=\linewidth]{figures/stm_effect_income.pdf}
%\end{figure}


\end{document}





%\begin{enumerate}
%    \item Design
%    \begin{enumerate}
%        \item Choosing the sample
%        \item Finding URLs
%        \item list of .gov websites
%        \item Finding supporting data
%    \end{enumerate}
%    \item Scraping
%        \begin{enumerate}
%            \item wget
%            \item headless browser/Selenium
%            \item Beautifulsoup/rvest
%            \item APIs (httr)
%            \item Wayback Machine
%        \end{enumerate}
%    \item Pre-processing
%        \begin{enumerate}
%            \item Determining document filetype
%            \item File conversion
%            \item Conventional preprocessing (lowercase, numbers, punctuation)
%            \item Stemming and lemmatization
%            \item spellchecking
%            \item Dealing with duplicate text \& html documents in particular
%        \end{enumerate}
%    \item Analysis
%        \begin{enumerate}
%            \item LDA
%            \item Other topic models (structural, author, dynamic -- maybe?)
%            \item SVM (+ other machine learning classifiers?)
%            \item Fightin Words
%        \end{enumerate}
%\end{enumerate}
%
%
%
%
%
%
%% latex table generated in R 3.3.3 by xtable 1.8-2 package
%% Wed Mar 22 11:32:43 2017
%%\begin{table}[ht]
%%    \centering
%%    \begin{tabular}{lrrlrrl}
%%        \hline
%%        City & DemVotes & RepVotes & Winner & Change & Pop15 & url \\ 
%%        \hline
%%        Attica &  & 187 & Republican & 0 & 3117 & https://attica-in.gov/ \\ 
%%        Connersville & 1005 & 995 & Democratic & 1 & 13010 & http://connersvillecommunity.com/ \\ 
%%        Frankfort &  & 1748 & Republican & 0 & 16060 & http://frankfort-in.gov/ \\ 
%%        Huntingburg & 447 & 793 & Republican & 0 & 6035 & http://www.huntingburg-in.gov/ \\ 
%%        Indianapolis & 92830 & 56661 & Democratic & 1 & 862781 & http://www.indy.gov \\ 
%%        Lake Station & 1483 & 227 & Democratic & 0 & 12054 & http://www.lakestation-in.gov/ \\ 
%%        Linton & 785 & 692 & Democratic & 0 & 5284 & http://www.linton-in.gov/ \\ 
%%        Madison & 1192 & 1915 & Republican & 0 & 12040 & http://www.madison-in.gov/ \\ 
%%        Mitchell & 229 & 495 & Republican & 1 & 4252 & http://mitchell-in.com/ \\ 
%%        Monticello & 0 &  & Democratic & 0 & 5322 & http://www.monticelloin.gov/ \\ 
%%        North Vernon & 679 & 697 & Republican & 1 & 6619 & http://www.northvernon-in.gov/ \\ 
%%        Richmond & 3421 & 2731 & Democratic & 0 & 35854 & http://www.richmondindiana.gov/ \\ 
%%        Rockport & 286 & 272 & Democratic & 1 & 2223 & http://www.cityofrockport-in.gov/ \\ 
%%        South Bend & 8515 & 2074 & Democratic & 0 & 101516 & https://www.southbendin.gov/ \\ 
%%        Union City & 338 & 440 & Republican & 0 & 3447 & http://www.unioncity-in.gov/ \\ 
%%        Winchester & 606 & 524 & Democratic & 1 & 4769 & http://www.winchester-in.gov/ \\ 
%%        \hline
%%    \end{tabular}
%%    \caption{} 
%%\end{table}
%
%\subsection{Research Design}
%
%\begin{table}[ht]
%    \centering
%    \begin{tabular}{llr}
%        \hline
%        Variable & Unit & Source \\
%        \hline
%        Population size & 1000 people & Census \\
%        Population growth last 5 years & Percent & Census \\
%        Type of economy (agriculture/industry/services) & ? & Census \\
%        Economic performance (GDP?) & \$ & Census \\
%        Party of mayor before election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
%        Party of mayor after election & Rep/Dem/(Ind) & in.gov/sos/elections/ \\
%        Change of party control & 0/1 & in.gov/sos/elections/ \\
%        Presidential vote 2012 in county & Percent Rep & ? (but I have the data) \\
%        Unemployment rate & Percent & Census \\
%        Broadband speed & Avg. Mbps DL & broadbandmap.gov \\
%        \hline
%    \end{tabular}
%    \caption{List of covariates} 
%\end{table}
%
%
%
%\begin{enumerate}
%\item Corpus:
%\begin{enumerate}
%\item Last snapshots before the election (November 3, 2015 in Indiana; tbd. in Louisiana (probably February))
%\item First snapshot that is at least 2 months after the new government's inauguration (which is in January for Indiana, May for Louisiana)
%\end{enumerate}
%\item Preprocessing:
%\begin{enumerate}
%\item restrict corpus to:
%\begin{enumerate}
%\item documents belonging to cities in which a change of power occurred
%\item documents that were added, deleted or changed between the two snapshots
%\end{enumerate}
%\item words to lowercase
%\item remove punctuation
%\item stemming (Porter stemming algorithm?)
%\item Remove stop words (regular list of stop words is enough, since we use an asymmetric prior)
%\end{enumerate}
%\item Apply Grimmer's expressed agenda model to the corpus
%\begin{enumerate}
%\item Asymmetric prior
%\item Each document can have only one topic (in contrast to the author-topic model)
%\item Cities $i = 1,..., n = 15$
%\item Topic $k(k = 1,..., K )$
%\item Documents $j(j = 1,...,D_i)$ from city i
%\item Party covariate in the prior, where the deleted and unmodified documents are coded as from the first, and the added and modified documents from the second party
%\end{enumerate}
%\item Results
%\begin{enumerate}
%\item Label topics using Grimmer's automatic cluster labeling method, based on most commonly used words in documents belonging to topic
%\item Evaluate topics
%\end{enumerate}
%\end{enumerate}
%
%Validation:
%
%\begin{itemize}
%\item Do the above for cities in which no change of power occurred.
%\item Check whether there is higher than average turnover around the new year by comparing changes to non-election years (and also Louisiana, where elections are later).
%\item Check how long documents stay on websites on average. Use websites with a lot of snapshots for this (these exist for both small and large cities).
%\end{itemize}
%
%Problem with using this model: Grimmer's expressed agenda model uses Senators as the actors. Senators is also who he is substantively interested in. For us, the equivalent to Senators is cities. However, we care about parties, not cities.
%
%\subsection{Survival model}
%The existence of individual documents on municipal government websites can be though of as a survival process. No document stays on a website forever, and it appears to be a reasonable assumption that as documents get older and thus less relevant, they get replaced. The factors determining the steepness of the survival curve are the topic - fire safety regulations likely stay up longer than a bulletin on the annual spring banquet - and the change of party control after an election.
%
%\begin{quote}
%\textit{H1}: The older a document, the more likely it is to be removed.
%\end{quote}
%
%$S(t)$ has a downward slope. Admittedly, this is almost impossible not to be true. Also, test proportional, rising and falling hazard models.
%
%\begin{quote}
%\textit{H2}: Documents pertaining to administrative matters are less likely to be removed.
%\end{quote}
%
%Introduce a categorical variable for the top 10(?) topics. A negative coefficient for administrative topics would support this hypothesis.
%
%\begin{quote}
%\textit{H3}: Documents introduced by the opposing party are more likely to be removed.
%\end{quote}
%
%Introduce two variables into the survival model: One variable indicating which party has introduced a document, and a time-varying variable describing which party is currently in government. The hypothesis is tested through an interaction term between the two.
%
%\begin{quote}
%\textit{H4a}: Democrats are more likely to remove documents with topics pertaining to private enterprise, private schools.
%\end{quote}
%
%Interaction term between party in power and categorical topic variable.
%
%\begin{quote}
%\textit{H4b}: Republicans are more likely to remove documents with topics pertaining to social justice, equality, taxes, public schools, etc.
%\end{quote}
%
%Interaction term between party in power and categorical topic variable.
%
%\begin{quote}
%\textit{H5}: In line with their commitment to small government, Republicans are more likely than Democrats to remove documents.
%\end{quote}
%
%Party in power variable.\\
%
%
%This model will take up a lot of degrees of freedom. The rarity of snapshots for some cities might be a problem. Documents being changed and being removed can be modeled as competing risks.
%
%
%\begin{equation} 
%\label{eq1}
%\begin{split}
%Y & = \text{Party that introduced the document} \\
% & + \text{Party that is currently in power} \\
% & + \text{Topic 1, topic 2, ..., topic k} \\
% & + \text{Party that is currently in power} \times \text{Topic 1, topic 2, ..., topic k} \\
% & + \text{Days since start of mayoral term (control)}
%\end{split}
%\end{equation}
%
%
%
%
%
%After the counties, townships and cities that cannot be matched to the Census data\footnote{There are five cities that are not contained in the Census data} and duplicate websites (some cities have more than one website) are removed, 1813 domains/cities remain.
%
%These cities contain 90,616,865 people, and thus about 28\% of the U.S. population (see figure 1).
%
%\begin{figure}[htp]
%    \centering
%    \caption{Percentage of state population covered.}
%    \includegraphics[width=0.9\linewidth,height=0.9\textheight]{figures/coverage_states.pdf}
%\end{figure}
%
%We use the resulting list of websites to acccess their copies stored in the Internet Archive's Wayback Machine. To this end, we rely on the Ruby Gem 'Wayback Machine Downloader'\footnote{https://github.com/hartator/wayback-machine-downloader} (WbMD). We supply the URL that each .gov website redirects to to the WbMD, which then downloads every file present in the WbM from a snapshot in October 2016, or, if not available, as soon as possible after this point.
%
%<Note: We have not actually done this last step for all websites (however, the R script which runs the Ruby package is already set up to do so once we need to). Instead 10 websites were randomly sampled from an older version of the GSA list, which still contained counties and townships, which is why one of the 10 websites is from Dutchess County, NY.>
%
%It would be fine to focus on Indiana as a case. First, we need to answer some preliminary questions about the data.
%
%\begin{enumerate}
%
%\item For what percentage and number of IN cities can we find data from the WBM?
%\item For how many election cycles can we find political leadership data for these matched cities?
%\item In what number and percentage of cities is the local leadership majority Republican? 
%\item Relatedly, in a typical election cycle, for how many cities do we see a transition in party leadership (i.e., a shift from majority D (R) to majority (R) D). 
%
%\end{enumerate}
%
%\begin{enumerate}
%    
%    \item 30 cities, with a combined population of 1,180,435. However, since only cities (as opposed to towns and villages) hold mayoral elections, only 16 of these, with a combined population of 1,094,383 can be matched to the election data.
%    \item 2015, 2011, 2007, 2003.
%    \item Of the 16 cities, 7 have Republican mayors after the 2015 elections.
%    \item In 6 cases, a shift of party control occurs, with 4 of these being Republican --> Democratic. 
%    
%\end{enumerate}
%
%
%
%
%\section{Running Application: Party Differences in Municipal Websites}
%
%
%\begin{landscape}
%\begin{table}[htbp]
%    %\caption{}
%    \begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%        \hline
%        Names & Year & Journal & Findings & Important? \\ \hline
%        Benedictis-Kessner, Justin De
%        Warshaw, Christopher & 2016 & JOP* & Regression discontinuity design. Democratic mayors spend more (but it is unclear on what, not the typical Democratic issue-areas), issue more debt, pay more interest & Yes \\ \hline
%        Caughey, Devin
%        Warshaw, Christopher
%        Xu, Yiqing & 2015 & Working Paper & Regression discontinuity design. Partisan composition of state governments affects state policy liberalism (composite index for the areas of social welfare, taxation, labor, civil rights, womenâs rights, moral legislation, family planning, environment). & Somewhat \\ \hline
%        Einstein, Katherine Levine
%        Kogan, Vladimir & 2015 & Urban Affairs Review & Cities with more Democratic citizens spend more; more progressive (rather than regressive) forms of taxation; pursue intergov. aid more; spend more on police, fire, parks \& recreation & Somewhat \\ \hline
%        Einstein, Katherine Levine
%        Glick, David M. & 2015 & Working Paper & Survey of 72 mayors. Unlike Republican mayors, roughly half of Democrats seem to agree that cities should aim to reduce inequality. Democratic mayors also seem to favor redistribution to accomplish that goal. & Somewhat \\ \hline
%        Kiewiet, D Roderick
%        Mccubbins, Mathew D & 2014 & Annual Review & City budgets have been severeley constrained since the Great Recession. Spending has thus decreased in general. Lack of funds means that there is not much discretion for partisanship. & Somewhat \\ \hline
%        Tausanovitch, Chris
%        Warshaw, Christopher & 2014 & APSR* & Cities are responsive (taxes, expenditures, regressiveness of taxation) to citizens' conservatism/liberalism. Partisan elections do not make cities more or less responsive. & Yes \\ \hline
%        GuillamÃ³n, Ma Dolores
%        Bastida, Francisco
%        Benito, Bernardino & 2013 & European Journal of Law and Economics & Police spending in Spain. Conservative parties spend more on police. Spending is higher before elections. Also contains a useful overview of the literature. & Yes \\ \hline
%    \end{tabular}
%    \label{}
%\end{table}
%\end{landscape}
%
%\begin{landscape}
%    \begin{table}[htbp]
%        %\caption{}
%        \begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%            \hline
%            Names & Year & Journal & Findings & Important? \\ \hline
%            Gerber, Elisabeth R. & 2013 & Cityscape & Partisanship of both citizens and elected city officials separately affect climate policy. & Yes \\ \hline
%            SolÃ©-OllÃ©, Albert
%            Viladecans-Marsal, Elisabet & 2013 & Journal of Urban Economics & Spanish cities. The authors "employ a regression discontinuity design to document that cities controlled by left-wing parties convert much less land from rural to urban uses than is the case in similar cities con- trolled by the right". Partisanship might also affect housing construction and price growth. & Yes \\ \hline
%            Gerber, Elisabeth R.
%            Hopkins, Daniel J. & 2011 & AJPS & Regression discontinuity design. Democratic mayors spend less on public safety. All other policy areas (including taxation) are unaffected. & Yes \\ \hline
%            Trounstine, Jessica & 2010 & Annual Review & Race and ethnicity in local elections (not relevant to us). Partisan elections have higher turnout; non-partisan elections still tend to have some partisanship in them because voters learn about party of candidates from media. Non-partisan elections favor Republicans/upper class. Mixed evidence for whether partisanship of mayor is important for policy. & Somewhat \\ \hline
%            Palus, Christine Kelleher & 2010 & State and Local Government Review & Ideology (liberal/conservative) of citizen is well represented by gov. spending in five areas: (1) community development, housing, and conservation, (2) health and human services, (3) culture, the arts, and recreation, (4) environmental programs, and (5) transportation. & Somewhat \\ \hline
%            Ferreira, Fernando
%            Gyourko, Joseph & 2009 & The Quarterly Journal of Economics & Regression discontinuity design. Null results for spending and city gov. size with regard to mayor partisanship. & Yes \\ \hline
%            Ansolabehere, Stephen
%            Snyder, James M. & 2006 & Scandinavian Journal of Economics & Despite the journal, this is about the U.S. The important finding (for us) is the fact that counties whose government is controlled by the same party as the state government, receive more funding (county's share of state transfers, normalized by county pop.) from the state. & Somewhat \\ \hline
%            Murphy, Russell D. & 2002 & Annual Review & Not useful. Too philosophical; mostly cites papers written a hundred years ago. Also exclusively about larger cities. & No \\ \hline
%        \end{tabular}
%        \label{}
%    \end{table}
%\end{landscape}
%
%\begin{landscape}
%    \begin{table}[htbp]
%        %\caption{}
%        \begin{tabular}{|p{2cm}|c|p{3cm}|p{12cm}|l|}
%            \hline
%            Names & Year & Journal & Findings & Important? \\ \hline
%            Armstrong, Cory L. & 2011 & Government Information Quarterly & Comparison of county and school board websites in Florida (where the two align) with regard to transparency (presence or absence of public records). Manual content analysis (undergrads told to look around for 15 minutes). School board websites, more professional websites, and websites in Republican-dominated counties are found to be more transparent. & Yes \\ \hline
%            Cegarra-Navarro, Juan
%            PachÃ³n, JosÃ©
%            Cegarra, JosÃ© & 2012 & International Journal of Information Management & Survey of Spanish municipal government officials (specifically, the city website managers). Respondents are asked about the features of their websites, the level of civic engagement and the size of their municipality. More sophisticated websites are correlated with greater civic engagement and greater use of e-government functions. & Yes \\ \hline
%            Dolson, Jordan
%            Young, Robert & 2012 & Canadian Journal of Urban Research & Determinants of website content. Three categories: e-content (city information on website), e-participation, social media use. Tables on page 15 show frequencies of these categories across sites, and might be useful to inform our topics. Larger cities have better websites. Population growth and immigration are also tested, but the findings are somewhat inconclusive. & Yes \\ \hline
%            Feeney, Mary K.
%            Brown, Adrian & 2017 & Government Information Quarterly & 500 U.S. city websites at two points in time (2010-2014). Count model of website features regarding information, e-services, utilities, transparency and civic engagement. Having a larger population leads to more features. Relying on a website contractor leads to more information and transparency. The authors say that mayor-councils are negatively correlated with website sophistication, but their regression tables state the opposite. & Yes \\ \hline
%            Kaylor, Charles
%            Deshazo, Randy
%            Van Eck, David & 2001 & Government Information Quarterly & Model of best practices of e-government. Table 1 lists a number of possible ways this manifests, could be useful for our theory. & Somewhat \\ \hline
%            Ansolabehere, Stephen
%            Urban, Florian & 2002 & Cities & Websites of 20 major cities across the world. Is website content correlated with city characteristics? Not particularly systematic, and the findings are inconclusive. & Somewhat \\ \hline
%            Jeffres, Leo W.
%            Lin, Carolyn A. & 2006 & Journal of Computer-Mediated Communication & 50 largest metropolitan areas in the U.S. Features include information about city, opportunities for citizen feedback, galleries of photos, links, etc.  Purely descriptive analysis, doesn't contain anything that isn't covered in any of the other aricles. & No \\ \hline
%        \end{tabular}
%        \label{}
%    \end{table}
%\end{landscape}
%
%\subsection{Informative Dirichlet model}
%%However, \cite{Monroe2008} advise against using these types of methods in this context because they get the data generation process backwards: Our theory assumes that party leads to variation in writing, and yet we rely on the documents to predict party, in spite of the fact that we actually have perfect knowledge of it.
%
%For the analysis of the data, we present two approaches, the first being the informative dirichlet model developed by \citep{Monroe2008}. This approach aims to account for the fact that some words naturally occur more than others by applying a Dirichlet prior based on the distribution of words in random text. Table \ref{tabFightinIN} shows the top words for both Democrats and Republicans - and accomplishes, to some extent, the goal of \citep{Monroe2008} of banishing frequent words from this list and supplanting them with text with greater semantic, and in our case, partisan meaning. 
%
%In Indiana, Democrats exhibit a preference for words related to public finance, such as 'fund', 'budget', or 'tax', indicative of a greater willingness to emphasize the city's efforts to raise and spend money. This finding is consistent with \citep{Einstein2015}, who show that Democratic mayors tend to favor greater spending. Beyond the focus on public finance, the words preferably used by Democrats do not fall into any particularly congruent categories, and largely sort into various areas related to city administration - i.e. `council', `services', `budget', `committee', `contract', etc. If there is theme around the words preferred by Republicans, it seems to center around city planning - street, fire, water, building, construction, park. These words suggest that the hands-off approach favored by Republicans results in a focus on supporting infrastructure and logistics.
%
%%Partisan top words - fightin words Indiana
%\input{tables/fightinwordsIN.tex} %\ref{tabFightinIN}
%
%For Lousiana, the results (see table \ref{tabFightinLA}) are less coherent. Only one of the finance-related terms appears again for Democrats - specifically `fund', although `rate' might also be used in a financial context. Beyond that, some focus on a `historic' `'district of a city seems evident, as is the use of some words - `infrastructure', `water', `building' that were used for Republicans in Indiana. Conversely, Republicans are now missing these words, and their preferred terms generally do not seem to follow any particular theme.
%
%%Partisan top words - fightin words Louisiana
%\input{tables/fightinwordsLA.tex} %\ref{tabFightinLA}
%
%The weakness of the fightin' words method is evident here, as a list of words does not necessarily provide sufficient information to glean preferred topics from. This is especially the case when the texts are spread across a broad number of issue-areas, with little semantic similarity. In \citep{Monroe2008}, the authors focus on the fairly constrained corpus of U.S. Senate speeches with respect to abortion - our context, by comparison, is far more eclectic.








